#![allow(deprecated)]

//! ME(b,L) SpartanCircuit implementation for direct Spartan2 SNARK integration
//!
//! This module implements the **real Spartan2 integration** using the `SpartanCircuit` trait.
//! This is the **official public API** approach, not a workaround.
//! 
//! ## Architecture
//!
//! 1. **`SpartanCircuit<E>` Implementation**: Uses bellpepper `ConstraintSystem`
//! 2. **Automatic R1CS Generation**: Spartan2 handles constraint matrix construction
//! 3. **Production SNARK API**: `setup() ‚Üí prep_prove() ‚Üí prove() ‚Üí verify()`
//! 4. **Hash-MLE PCS Backend**: Integrated automatically via Engine selection
//!
//! ## Constraints Implemented
//!
//! 1. **Ajtai commitment binding**: `‚ü®L_{r,i}, vec(Z)‚ü© = c_{r,i}`
//! 2. **ME evaluations**: `‚ü®v_j^{(‚Ñì)}, Z_row[r]‚ü© = y_j^{(‚Ñì)}[r]` 
//! 3. **Fold digest binding**: Ensures transcript security between phases
//!
//! ## Variable Layout (Bellpepper Style)
//!
//! - **Public Inputs**: `(c_coords, y_limbs, fold_digest_limbs, challenges)`
//! - **Private Witness**: `vec(Z)` allocated as `AllocatedNum<E::Scalar>`
//! - **Shared Variables**: Empty for this circuit (no cross-circuit dependencies)

use anyhow::Result;
use bellpepper_core::{num::AllocatedNum, ConstraintSystem, SynthesisError};
use dashmap::DashMap;
use ff::Field;
use once_cell::sync::Lazy;
use p3_field::{PrimeField64, PrimeCharacteristicRing};
use p3_goldilocks::{Goldilocks, Poseidon2Goldilocks};
use p3_symmetric::{Permutation};
// RNG imports are now local to the static initializer
use spartan2::errors::SpartanError;
use spartan2::provider::GoldilocksMerkleMleEngine as E;
use spartan2::spartan::{R1CSSNARK, SpartanProverKey, SpartanVerifierKey};
use spartan2::traits::{circuit::SpartanCircuit, Engine, pcs::PCSEngineTrait, snark::R1CSSNARKTrait};
use std::sync::Arc;

// Real Spartan2 SNARK uses circuits directly, not R1CS shapes
// (the R1CS conversion happens internally)

use neo_ajtai::PP as AjtaiPP; // for later PP-backed binding
use neo_ccs::{MEInstance, MEWitness};

/// Circuit fingerprint key for caching SNARK setup and preparation
/// SECURITY CRITICAL: This must fingerprint the actual constraint constants/wiring, 
/// not just sizes. Two circuits with same dimensions but different matrices 
/// CANNOT safely share keys.
#[derive(Clone, Debug, Eq, PartialEq, Hash)]
pub struct CircuitKey([u8; 32]);

impl CircuitKey {
    pub fn from_circuit(circuit: &MeCircuit) -> Self {
        // Use Poseidon2 over Goldilocks for program fingerprinting (consistent with Neo transcript)
        const WIDTH: usize = 16;
        
        let mut state = Vec::<Goldilocks>::new();
        
        // Domain separation tag and versioning (encode as field elements)
        state.push(Goldilocks::from_u64(0x4e454f5f50524f)); // "NEO_PRO" as u64  
        state.push(Goldilocks::from_u64(0x4752414d5f4b4559)); // "GRAM_KEY" as u64
        state.push(Goldilocks::from_u64(2)); // version 2 (excludes public input values)
        
        // Base parameters that alter constraint generation
        state.push(Goldilocks::from_u64(circuit.me.base_b));
        state.push(Goldilocks::from_u64(circuit.wit.z_digits.len() as u64));
        state.push(Goldilocks::from_u64(circuit.me.c_coords.len() as u64));
        state.push(Goldilocks::from_u64(circuit.me.y_outputs.len() as u64));
        state.push(Goldilocks::from_u64(circuit.me.r_point.len() as u64));
        
        // CRITICAL: Hash all Ajtai rows (constraint matrix constants)
        if let Some(rows) = &circuit.wit.ajtai_rows {
            state.push(Goldilocks::from_u64(rows.len() as u64));
            for row in rows {
                state.push(Goldilocks::from_u64(row.len() as u64));
                for &coeff in row {
                    state.push(Goldilocks::from_u64(coeff.as_canonical_u64()));
                }
            }
        } else {
            state.push(Goldilocks::ZERO);
        }
        
        // CRITICAL: Hash all weight vectors (constraint matrix constants)
        state.push(Goldilocks::from_u64(circuit.wit.weight_vectors.len() as u64));
        for weight_vec in &circuit.wit.weight_vectors {
            state.push(Goldilocks::from_u64(weight_vec.len() as u64));
            for &coeff in weight_vec {
                state.push(Goldilocks::from_u64(coeff.as_canonical_u64()));
            }
        }
        
        // CIRCUIT STRUCTURE ONLY: Hash counts/lengths that affect circuit shape,
        // but NOT the values of public inputs (c_coords, y_outputs, r_point, fold_digest).
        // Public input values vary per instance and must not fragment the PK/VK cache.
        
        // Include counts/lengths that determine circuit structure
        state.push(Goldilocks::from_u64(circuit.me.c_coords.len() as u64));
        state.push(Goldilocks::from_u64(circuit.me.y_outputs.len() as u64)); 
        state.push(Goldilocks::from_u64(circuit.me.r_point.len() as u64));
        
        // CRITICAL: Include header digest (fold_digest) as it affects circuit behavior
        // Different header digests create different circuits that need different keys
        for chunk in circuit.fold_digest.chunks(8) {
            let mut bytes = [0u8; 8];
            bytes[..chunk.len()].copy_from_slice(chunk);
            state.push(Goldilocks::from_u64(u64::from_le_bytes(bytes)));
        }
        
        // Apply Poseidon2 permutation to get deterministic fingerprint
        // STABILITY: Create a singleton Poseidon2 instance for circuit fingerprinting
        // This avoids RNG-dependent construction and ensures fingerprint stability
        // across library versions and builds.
        static CIRCUIT_FINGERPRINT_POSEIDON2: Lazy<Poseidon2Goldilocks<16>> = 
            Lazy::new(|| {
                use rand_chacha::{ChaCha8Rng, rand_core::SeedableRng};
                // NOTE: This RNG usage is one-time on first use (lazy runtime init).
                // It yields deterministic constants per process for the fixed seed,
                // but may still change across library versions if parameter derivation changes.
                // For maximum stability across versions, prefer explicit fixed Poseidon2 parameters.
                const FINGERPRINT_SEED: u64 = 0x4E454F5F434B4559; // "NEO_CKEY" - distinct from transcript
                let mut rng = ChaCha8Rng::seed_from_u64(FINGERPRINT_SEED);
                Poseidon2Goldilocks::<16>::new_from_rng_128(&mut rng)
            });
        let poseidon2 = &*CIRCUIT_FINGERPRINT_POSEIDON2;
        
        // Pad to multiple of width (16 for Poseidon2)
        let mut padded_state = state;
        while padded_state.len() % WIDTH != 0 {
            padded_state.push(Goldilocks::ZERO);
        }
        
        // Hash in chunks of WIDTH, building up a running hash
        let mut final_hash = [Goldilocks::ZERO; WIDTH];
        for chunk in padded_state.chunks(WIDTH) {
            let mut chunk_array = [Goldilocks::ZERO; WIDTH];
            chunk_array[..chunk.len()].copy_from_slice(chunk);
            // Absorb previous hash state (Merkle-Damg√•rd style)
            for (i, &val) in final_hash.iter().enumerate() {
                chunk_array[i] += val;
            }
            poseidon2.permute_mut(&mut chunk_array);
            final_hash = chunk_array;
        }
        
        // Convert to 32-byte digest (use first 4 field elements)
        let mut digest = [0u8; 32];
        for (i, &elem) in final_hash[..4].iter().enumerate() {
            digest[i*8..(i+1)*8].copy_from_slice(&elem.as_canonical_u64().to_le_bytes());
        }
        
        Self(digest)
    }
    
    /// Extract the inner bytes for use in lean proof system
    pub fn into_bytes(self) -> [u8; 32] {
        self.0
    }
}

/// Global cache for SNARK proving keys (keyed by circuit fingerprint)
static PK_CACHE: Lazy<DashMap<CircuitKey, Arc<SpartanProverKey<E>>>> =
    Lazy::new(DashMap::new);

/// Global cache for SNARK verifying keys (keyed by circuit fingerprint)
static VK_CACHE: Lazy<DashMap<CircuitKey, Arc<SpartanVerifierKey<E>>>> =
    Lazy::new(DashMap::new);

// Note: Prepared SNARK caching would provide additional performance benefits,
// but requires determining the exact return type of prep_prove() from Spartan2.
// The PK/VK caching above provides the largest performance win.

/// ME(b,L) circuit
#[derive(Clone, Debug)]
pub struct MeCircuit {
    pub me: MEInstance,
    pub wit: MEWitness,
    /// Optional Ajtai PP; once you expose rows from PP, use this instead of wit.ajtai_rows
    pub pp: Option<std::sync::Arc<AjtaiPP<neo_math::Rq>>>,
    /// 32-byte fold digest (binds transcript / header)
    pub fold_digest: [u8; 32],
}

impl MeCircuit {
    pub fn new(me: MEInstance, wit: MEWitness, pp: Option<std::sync::Arc<AjtaiPP<neo_math::Rq>>>, fold_digest: [u8; 32]) -> Self {
        Self { me, wit, pp, fold_digest }
    }

    #[inline]
    fn digest_to_scalars(&self) -> Vec<<E as Engine>::Scalar> {
        self.fold_digest
            .chunks(8)
            .map(|chunk| {
                let mut b = [0u8; 8];
                b[..chunk.len()].copy_from_slice(chunk);
                <E as Engine>::Scalar::from(u64::from_le_bytes(b))
            })
            .collect()
    }

    /// v1: y ‚àà Fq; when you move y ‚àà K = Fq^2, split here.
    #[inline]
    #[allow(dead_code)]
    fn k_to_limbs(
        &self,
        x: p3_goldilocks::Goldilocks,
    ) -> (p3_goldilocks::Goldilocks, p3_goldilocks::Goldilocks) {
        (x, p3_goldilocks::Goldilocks::ZERO)
    }

    #[inline]
    #[allow(dead_code)]
    fn is_pow2(n: usize) -> bool {
        n.is_power_of_two()
    }
}

/// Public inputs order **must** mirror `encode_bridge_io_header()`:
/// (c_coords) || (y limbs) || (r_point) || (base_b) || (fold_digest limbs)
impl SpartanCircuit<E> for MeCircuit {
    fn public_values(&self) -> Result<Vec<<E as Engine>::Scalar>, SynthesisError> {
        // TRANSCRIPT CONSISTENCY FIX: Return the SAME values that inputize() creates
        // Both prover (via this method) and verifier (via U.validate) must absorb identical data
        let mut pv = Vec::new();

        // 1) Ajtai commitment coords (Fq) - SAME ORDER as inputize() calls
        for &c in &self.me.c_coords {
            pv.push(<E as Engine>::Scalar::from(c.as_canonical_u64()));
        }
        // 2) y outputs ‚Äî already flattened (K -> [F;2]) in adapter
        for &y_limb in &self.me.y_outputs {
            pv.push(<E as Engine>::Scalar::from(y_limb.as_canonical_u64()));
        }
        // 3) challenge r (Fq^m)
        for &r in &self.me.r_point {
            pv.push(<E as Engine>::Scalar::from(r.as_canonical_u64()));
        }
        // 4) base dimension b
        pv.push(<E as Engine>::Scalar::from(self.me.base_b as u64));
        
        // 5) Pad to power-of-2 BEFORE digest - CRITICAL: match encode_bridge_io_header() order
        let current_count_without_digest = self.me.c_coords.len() + self.me.y_outputs.len() + 
                                           self.me.r_point.len() + 1;
        let digest_scalars = self.digest_to_scalars();
        let total_needed = current_count_without_digest + digest_scalars.len();
        let target_count = total_needed.next_power_of_two();
        let padding_needed = target_count - total_needed;
        
        // Add padding zeros BEFORE digest to match encode_bridge_io_header() order
        for _ in 0..padding_needed {
            pv.push(<E as Engine>::Scalar::ZERO);
        }
        
        // 6) fold digest limbs LAST (after padding)
        pv.extend(digest_scalars);

        if padding_needed > 0 {
            eprintln!("üîß TRANSCRIPT FIX: public_values() padded from {} to {} (matches encode_bridge_io_header)", total_needed, pv.len());
        }
        Ok(pv)
    }

    fn shared<CS: ConstraintSystem<<E as Engine>::Scalar>>(
        &self,
        _cs: &mut CS,
    ) -> Result<Vec<AllocatedNum<<E as Engine>::Scalar>>, SynthesisError> {
        eprintln!("‚úÖ shared(): EMPTY (Hash-MLE single-blind strategy)");
        Ok(vec![])
    }

    fn precommitted<CS: ConstraintSystem<<E as Engine>::Scalar>>(
        &self,
        _cs: &mut CS,
        _shared: &[AllocatedNum<<E as Engine>::Scalar>],
    ) -> Result<Vec<AllocatedNum<<E as Engine>::Scalar>>, SynthesisError> {
        eprintln!("üîß precommitted(): EMPTY (avoiding multiple blinds for Hash-MLE)");
        eprintln!("   Hash-MLE PCS requires exactly 1 blind, so we use only REST segment");
        Ok(vec![])
    }

    fn num_challenges(&self) -> usize { 0 }

    fn synthesize<CS: ConstraintSystem<<E as Engine>::Scalar>>(
        &self,
        cs: &mut CS,
        _shared: &[AllocatedNum<<E as Engine>::Scalar>],  // empty now
        _z_pre: &[AllocatedNum<<E as Engine>::Scalar>],  // empty
        _challenges: Option<&[<E as Engine>::Scalar]>,
    ) -> Result<(), SynthesisError> {
        // SECURITY: refuse to synthesize an unbound circuit
        let has_ajtai_rows = self.wit.ajtai_rows.as_ref().map_or(false, |r| !r.is_empty());
        let has_pp = self.pp.is_some();
        
        if !has_ajtai_rows && !has_pp {
            eprintln!("‚ùå SECURITY: Ajtai rows missing AND no PP provided; refusing to synthesize unbound circuit.");
            return Err(SynthesisError::AssignmentMissing);
        }

        let mut z_vars = Vec::<AllocatedNum<<E as Engine>::Scalar>>::new();
        
        // 1) Allocate REST witness variables exactly in the z_digits order
        #[cfg(feature = "debug-logs")]
        eprintln!("üîç Witness allocation (REST segment - z_digits first):");
        
        // CRITICAL FIX: Always allocate witness variables, not just when debug-logs is enabled
        for (i, &z) in self.wit.z_digits.iter().enumerate() {
            let val = if z >= 0 {
                <E as Engine>::Scalar::from(z as u64)
            } else {
                -<E as Engine>::Scalar::from((-z) as u64)
            };
            #[cfg(feature = "debug-logs")]
            eprintln!("   z_vars[{}] = {} (from z_digits[{}] = {})", i, val.to_canonical_u64(), i, z);
            
            z_vars.push(AllocatedNum::alloc(cs.namespace(|| format!("REST_Z[{i}]")), || Ok(val))?);
        }
        
        // 2) Pad REST (only REST) to a power-of-two for Hash-MLE
        let orig_rest = z_vars.len();
        let target_rest = if orig_rest <= 1 { 1 } else { orig_rest.next_power_of_two() };
        for i in orig_rest..target_rest {
            z_vars.push(AllocatedNum::alloc(cs.namespace(|| format!("REST_Z_pad[{i}]")), 
                || Ok(<E as Engine>::Scalar::ZERO))?);
        }
        eprintln!("üîç synthesize(): REST padded from {} to {} (power-of-two)", orig_rest, z_vars.len());
        debug_assert!(z_vars.len().is_power_of_two());
        
        let mut n_constraints = 0usize;
        let to_s = |x: p3_goldilocks::Goldilocks| <E as Engine>::Scalar::from(x.as_canonical_u64());

        // Enforce padded limbs are exactly zero (avoid unconstrained witness coords)
        if target_rest > orig_rest {
            let mut pad_ct = 0usize;
            for i in orig_rest..target_rest {
                cs.enforce(
                    || format!("pad_zero_{}", i),
                    |lc| lc + z_vars[i].get_variable(),
                    |lc| lc + CS::one(),
                    |lc| lc, // == 0
                );
                pad_ct += 1;
            }
            n_constraints += pad_ct;
            eprintln!("üîß Enforced {} padded REST limbs to zero", pad_ct);
        }

        // (RANGE) Enforce |Z_i| < b  (digits ‚àà {-(b-1), ‚Ä¶, (b-1)})
        // For small b (e.g., b=2 => {-1,0,1}), constrain with a product polynomial.
        // For b=2: v*(v-1)*(v+1) == 0.
        for (i, z) in z_vars.iter().enumerate().take(orig_rest) { // Only apply to original witness, not padding
            if self.me.base_b == 2 {
                let one = <E as Engine>::Scalar::ONE;

                // (z)*(z-1)*(z+1) == 0
                // Implement as three constraints using a temp:
                let z_minus_one = AllocatedNum::alloc(cs.namespace(|| format!("z_minus_one_{}", i)), || {
                    Ok(z.get_value().ok_or(SynthesisError::AssignmentMissing)? - one)
                })?;
                cs.enforce(
                    || format!("link_z_minus_one_{}", i),
                    |lc| lc + z.get_variable(),
                    |lc| lc + CS::one(),
                    |lc| lc + z_minus_one.get_variable() + (one, CS::one()),
                );
                let z_plus_one = AllocatedNum::alloc(cs.namespace(|| format!("z_plus_one_{}", i)), || {
                    Ok(z.get_value().ok_or(SynthesisError::AssignmentMissing)? + one)
                })?;
                cs.enforce(
                    || format!("link_z_plus_one_{}", i),
                    |lc| lc + z.get_variable(),
                    |lc| lc + CS::one(),
                    |lc| lc + z_plus_one.get_variable() + (-one, CS::one()),
                );
                // w := z * (z-1)
                let w = AllocatedNum::alloc(cs.namespace(|| format!("w_{}", i)), || {
                    let zv = z.get_value().ok_or(SynthesisError::AssignmentMissing)?;
                    let zm1 = z_minus_one.get_value().ok_or(SynthesisError::AssignmentMissing)?;
                    Ok(zv * zm1)
                })?;
                cs.enforce(
                    || format!("w=z*(z-1)_{}", i),
                    |lc| lc + z.get_variable(),
                    |lc| lc + z_minus_one.get_variable(),
                    |lc| lc + w.get_variable(),
                );
                // w * (z+1) == 0
                cs.enforce(
                    || format!("range_b2_{}", i),
                    |lc| lc + w.get_variable(),
                    |lc| lc + z_plus_one.get_variable(),
                    |lc| lc,
                );
                n_constraints += 4;
            } else {
                // Generic small-b range: ‚àè_{t=-(b-1)}^{b-1} (z - t) == 0
                // Keep it simple for now (b is small).
                let mut acc = AllocatedNum::alloc(
                    cs.namespace(|| format!("range_acc_init_{}", i)),
                    || Ok(<E as Engine>::Scalar::ONE)
                )?;
                // CRITICAL: anchor the accumulator to ONE
                // Enforce acc * 1 = 1  ‚áí  acc == 1
                cs.enforce(
                    || format!("range_acc_init_is_one_{}", i),
                    |lc| lc + acc.get_variable(),
                    |lc| lc + CS::one(),
                    |lc| lc + CS::one(),
                );
                n_constraints += 1;
                for t in (-(self.me.base_b as i64 - 1))..=(self.me.base_b as i64 - 1) {
                    let c = if t >= 0 { 
                        <E as Engine>::Scalar::from(t as u64) 
                    } else { 
                        -<E as Engine>::Scalar::from((-t) as u64) 
                    };
                    // lin := (z - t)
                    let lin = AllocatedNum::alloc(cs.namespace(|| format!("range_lin_{}_{}", i, t)), || {
                        let zv = z.get_value().ok_or(SynthesisError::AssignmentMissing)?;
                        Ok(zv - c)
                    })?;
                    // enforce lin == z - t
                    cs.enforce(
                        || format!("lin=z-{}_digit_{}", t, i),
                        |lc| lc + z.get_variable(),
                        |lc| lc + CS::one(),
                        |lc| lc + lin.get_variable() + (c, CS::one()),
                    );
                    // acc := acc * lin
                    let new_acc = AllocatedNum::alloc(cs.namespace(|| format!("range_acc_next_{}_{}", i, t)), || {
                        Ok(acc.get_value().ok_or(SynthesisError::AssignmentMissing)? *
                           lin.get_value().ok_or(SynthesisError::AssignmentMissing)?)
                    })?;
                    cs.enforce(
                        || format!("acc_mul_{}_{}", i, t),
                        |lc| lc + acc.get_variable(),
                        |lc| lc + lin.get_variable(),
                        |lc| lc + new_acc.get_variable(),
                    );
                    acc = new_acc;
                    n_constraints += 2;
                }
                // acc must be 0
                cs.enforce(
                    || format!("range_final_zero_{}", i),
                    |lc| lc + acc.get_variable(),
                    |lc| lc + CS::one(),
                    |lc| lc,
                );
                n_constraints += 1;
            }
        }

        // (A) Ajtai binding: <L_i, Z> = c_i  (rows are constants in v1)
        // Use streaming approach if ajtai_rows is None but PP is available
        if let Some(rows) = &self.wit.ajtai_rows {
            // Traditional approach: use pre-materialized rows
            let n = core::cmp::min(rows.len(), self.me.c_coords.len());
            for i in 0..n {
                let row = &rows[i];
                let upto = core::cmp::min(row.len(), z_vars.len());
                let c_scalar = <E as Engine>::Scalar::from(self.me.c_coords[i].as_canonical_u64());

                // üîß DEBUG: Compute expected LHS value for constraint debugging
                let mut expected_lhs = <E as Engine>::Scalar::ZERO;
                for j in 0..upto {
                    let row_coeff = to_s(row[j]);
                    let z_val = if j < self.wit.z_digits.len() {
                        let z = self.wit.z_digits[j];
                        if z >= 0 { <E as Engine>::Scalar::from(z as u64) }
                        else { -<E as Engine>::Scalar::from((-z) as u64) }
                    } else {
                        <E as Engine>::Scalar::ZERO // padded variables
                    };
                    expected_lhs += row_coeff * z_val;
                }
                #[cfg(feature = "debug-logs")]
                eprintln!("üîç Ajtai constraint {}: computed LHS = {}, expected RHS = {}", 
                    i, expected_lhs.to_canonical_u64(), c_scalar.to_canonical_u64());

                cs.enforce(
                    || format!("ajtai_bind_{i}"),
                    |lc| {
                        let mut lc = lc;
                        for j in 0..upto {
                            lc = lc + (to_s(row[j]), z_vars[j].get_variable());
                        }
                        lc
                    },
                    |lc| lc + CS::one(),            // multiply by constant 1 (X side)
                    |lc| lc + (c_scalar, CS::one()),// right side also uses constant 1
                );
                n_constraints += 1;
            }
        } else if let Some(pp) = &self.pp {
            // üöÄ STREAMING APPROACH: Compute rows on-demand from PP to avoid memory cliff
            let n = self.me.c_coords.len();
            let z_len_padded = self.wit.z_digits.len();
            let z_len_original = pp.d * pp.m; // Original dimensions from PP
            
            for i in 0..n {
                // Compute single row on-demand using original dimensions
                let mut row = match neo_ajtai::compute_single_ajtai_row(pp, i, z_len_original, n) {
                    Ok(row) => row,
                    Err(e) => {
                        eprintln!("‚ùå Failed to compute Ajtai row {}: {}", i, e);
                        return Err(SynthesisError::AssignmentMissing);
                    }
                };
                
                // Pad the row with zeros to match the padded witness length
                if z_len_padded > z_len_original {
                    let pad_len = z_len_padded - z_len_original;
                    row.extend(std::iter::repeat(neo_math::F::ZERO).take(pad_len));
                }
                
                let upto = core::cmp::min(row.len(), z_vars.len());
                let c_scalar = <E as Engine>::Scalar::from(self.me.c_coords[i].as_canonical_u64());

                // üîß DEBUG: Compute expected LHS value for constraint debugging
                let mut expected_lhs = <E as Engine>::Scalar::ZERO;
                for j in 0..upto {
                    let row_coeff = to_s(row[j]);
                    let z_val = if j < self.wit.z_digits.len() {
                        let z = self.wit.z_digits[j];
                        if z >= 0 { <E as Engine>::Scalar::from(z as u64) }
                        else { -<E as Engine>::Scalar::from((-z) as u64) }
                    } else {
                        <E as Engine>::Scalar::ZERO // padded variables
                    };
                    expected_lhs += row_coeff * z_val;
                }
                #[cfg(feature = "debug-logs")]
                eprintln!("üîç Ajtai STREAMING constraint {}: computed LHS = {}, expected RHS = {}", 
                    i, expected_lhs.to_canonical_u64(), c_scalar.to_canonical_u64());

                cs.enforce(
                    || format!("ajtai_bind_streaming_{i}"),
                    |lc| {
                        let mut lc = lc;
                        for j in 0..upto {
                            lc = lc + (to_s(row[j]), z_vars[j].get_variable());
                        }
                        lc
                    },
                    |lc| lc + CS::one(),            // multiply by constant 1 (X side)
                    |lc| lc + (c_scalar, CS::one()),// right side also uses constant 1
                );
                n_constraints += 1;
            }
        }

        // Pattern A link constraints removed - they were incorrect (applied Ajtai rows to witness vars, not digits)
        // and expensive (O(d¬∑Œ∫) constraints). The RLC binder in the folding layer provides the correct binding.

        // (B) ME evals: <w_j, Z> = y_j  
        let m = core::cmp::min(self.wit.weight_vectors.len(), self.me.y_outputs.len());
        for j in 0..m {
            let wj = &self.wit.weight_vectors[j];
            let upto = core::cmp::min(wj.len(), z_vars.len());
            let _y_scalar = <E as Engine>::Scalar::from(self.me.y_outputs[j].as_canonical_u64());

            // üîß DEBUG: Compute expected LHS value for ME constraint debugging
            let mut expected_lhs = <E as Engine>::Scalar::ZERO;
            for k in 0..upto {
                let w_coeff = to_s(wj[k]);
                let z_val = if k < self.wit.z_digits.len() {
                    let z = self.wit.z_digits[k];
                    if z >= 0 { <E as Engine>::Scalar::from(z as u64) }
                    else { -<E as Engine>::Scalar::from((-z) as u64) }
                } else {
                    <E as Engine>::Scalar::ZERO // padded variables
                };
                expected_lhs += w_coeff * z_val;
            }
            #[cfg(feature = "debug-logs")]
            // eprintln!("üîç ME constraint {}: computed LHS = {}, expected RHS = {}", 
            //     j, expected_lhs.to_canonical_u64(), _y_scalar.to_canonical_u64());

            cs.enforce(
                || format!("me_eval_{j}"),
                |lc| {
                    let mut lc = lc;
                    for k in 0..upto {
                        lc = lc + (to_s(wj[k]), z_vars[k].get_variable());
                    }
                    lc
                },
                |lc| lc + CS::one(),               // multiply by constant 1
                |lc| lc + (_y_scalar, CS::one()),  // RHS uses constant 1
            );
            n_constraints += 1;
        }

        // CRITICAL SECURITY: Reject vacuous circuits that have no binding to the statement
        let has_ajtai_binding = has_ajtai_rows || has_pp;
        if !has_ajtai_binding && self.wit.weight_vectors.is_empty()
        {
            eprintln!("‚ùå SECURITY VIOLATION: Cannot synthesize circuit with no Ajtai binding AND no ME constraints.");
            eprintln!("   This would create a vacuous proof with no binding to the statement.");
            return Err(SynthesisError::AssignmentMissing);
        }

        // All witness variables are now allocated as REST with power-of-2 length

        // --- Ensure TOTAL number of constraints is a power of two (Hash‚ÄëMLE requirement)
        let total_constraints_so_far = n_constraints;
        let want_constraints = if total_constraints_so_far <= 1 { 1 } else { total_constraints_so_far.next_power_of_two() };
        let need_to_add = want_constraints - total_constraints_so_far;
        if need_to_add > 0 {
            // Add 1*1 = 1 tautologies using CS::one()
            for i in 0..need_to_add {
                cs.enforce(
                    || format!("pad_const_tautology_{i}"),
                    |lc| lc + CS::one(),
                    |lc| lc + CS::one(),
                    |lc| lc + CS::one(),
                );
            }
            eprintln!("üîß synthesize(): padded constraints {} ‚Üí {} (total with shared = {})", total_constraints_so_far, want_constraints, want_constraints);
        } else {
            eprintln!("‚ÑπÔ∏è  synthesize(): constraints = {} (total with shared = {} - power-of-two OK)", n_constraints, total_constraints_so_far);
        }

        // CRITICAL FIX: Create public inputs via inputize() for transcript fork compatibility
        // This replaces the old public_values() method approach
        
        // 1) Ajtai commitment coords (Fq)
        for (i, &c) in self.me.c_coords.iter().enumerate() {
            let c_val = <E as Engine>::Scalar::from(c.as_canonical_u64());
            let c_alloc = AllocatedNum::alloc(cs.namespace(|| format!("c_coord_{}", i)), || Ok(c_val))?;
            let _ = c_alloc.inputize(cs.namespace(|| format!("public_c_{}", i)));
        }
        
        // 2) y outputs ‚Äî already flattened (K -> [F;2]) in adapter
        for (i, &y_limb) in self.me.y_outputs.iter().enumerate() {
            let v = <E as Engine>::Scalar::from(y_limb.as_canonical_u64());
            let a = AllocatedNum::alloc(cs.namespace(|| format!("y_output_limb_{}", i)), || Ok(v))?;
            let _ = a.inputize(cs.namespace(|| format!("public_y_limb_{}", i)));
        }
        
        // 3) challenge r (Fq^m)
        for (i, &r) in self.me.r_point.iter().enumerate() {
            let r_val = <E as Engine>::Scalar::from(r.as_canonical_u64());
            let r_alloc = AllocatedNum::alloc(cs.namespace(|| format!("r_point_{}", i)), || Ok(r_val))?;
            let _ = r_alloc.inputize(cs.namespace(|| format!("public_r_{}", i)));
        }
        
        // 4) base dimension b
        let b_val = <E as Engine>::Scalar::from(self.me.base_b as u64);
        let b_alloc = AllocatedNum::alloc(cs.namespace(|| "base_b"), || Ok(b_val))?;
        let _ = b_alloc.inputize(cs.namespace(|| "public_base_b"));
        
        // 5) Pad to power-of-2 BEFORE digest - CRITICAL: match encode_bridge_io_header() order
        let current_count_without_digest = self.me.c_coords.len() + self.me.y_outputs.len() + 
                                           self.me.r_point.len() + 1;
        let digest_scalars = self.digest_to_scalars();
        let total_needed = current_count_without_digest + digest_scalars.len();
        let target_count = total_needed.next_power_of_two();
        let padding_needed = target_count - total_needed;
        
        // Add padding zeros BEFORE digest to match encode_bridge_io_header() order
        for i in 0..padding_needed {
            let zero_alloc = AllocatedNum::alloc(cs.namespace(|| format!("padding_{}", i)), || Ok(<E as Engine>::Scalar::ZERO))?;
            let _ = zero_alloc.inputize(cs.namespace(|| format!("public_padding_{}", i)));
        }
        
        // 6) fold digest limbs LAST (after padding)
        for (i, &digest_val) in digest_scalars.iter().enumerate() {
            let digest_alloc = AllocatedNum::alloc(cs.namespace(|| format!("digest_{}", i)), || Ok(digest_val))?;
            let _ = digest_alloc.inputize(cs.namespace(|| format!("public_digest_{}", i)));
        }
        
        if padding_needed > 0 {
            eprintln!("üîç inputize() padded from {} to {} public inputs (matches encode_bridge_io_header)", total_needed, target_count);
        }

        Ok(())
    }
}

/// Public API: setup keys (so you can cache them)
pub fn setup_me_snark(
    me: &MEInstance,
    wit: &MEWitness,
) -> Result<(SpartanProverKey<E>, SpartanVerifierKey<E>), SpartanError> {
    let circuit = MeCircuit::new(me.clone(), wit.clone(), None, me.header_digest);
    
    // Spartan2 SNARK setup expects the circuit directly
    R1CSSNARK::<E>::setup(circuit)
}

/// Public API: prove a real Spartan2 SNARK over Hash‚ÄëMLE (Poseidon2) with caching
pub fn prove_me_snark(
    me: &MEInstance,
    wit: &MEWitness,
) -> Result<(Vec<u8>, Vec<<E as Engine>::Scalar>, Arc<SpartanVerifierKey<E>>), SpartanError> {
    prove_me_snark_with_pp(me, wit, None)
}

/// Public API: prove a real Spartan2 SNARK with optional PP for streaming Ajtai rows
pub fn prove_me_snark_with_pp(
    me: &MEInstance,
    wit: &MEWitness,
    pp: Option<std::sync::Arc<neo_ajtai::PP<neo_math::Rq>>>,
) -> Result<(Vec<u8>, Vec<<E as Engine>::Scalar>, Arc<SpartanVerifierKey<E>>), SpartanError> {
    let circuit = MeCircuit::new(me.clone(), wit.clone(), pp, me.header_digest);
    
    // Assert PCS width is correct for Hash-MLE (binary hypercube)
    let pcs_width = <E as Engine>::PCS::width();
    #[cfg(feature = "debug-logs")]
    eprintln!("üîç ENGINE PCS WIDTH = {}", pcs_width);
    assert_eq!(pcs_width, 2, "Hash-MLE PCS must report width=2 (binary hypercube arity), got {}", pcs_width);
    
    // Generate circuit fingerprint for secure caching
    let circuit_key = CircuitKey::from_circuit(&circuit);
    
    #[cfg(feature = "debug-logs")]
    eprintln!("üîë Circuit fingerprint: {:?}", circuit_key);
    
    // 1. Get or create SNARK keys (with secure fingerprint-based caching)
    let (pk, vk) = if let Some(cached_pk) = PK_CACHE.get(&circuit_key) {
        let cached_vk = VK_CACHE.get(&circuit_key)
            .expect("VK must exist if PK exists");
        #[cfg(feature = "debug-logs")]
        eprintln!("‚úÖ Using cached SNARK keys for circuit fingerprint");
        (cached_pk.clone(), cached_vk.clone())
    } else {
        #[cfg(feature = "debug-logs")]
        eprintln!("üîç Setting up new SNARK keys for new circuit fingerprint");
        let (new_pk, new_vk) = R1CSSNARK::<E>::setup(circuit.clone())?;
        let pk_arc = Arc::new(new_pk);
        let vk_arc = Arc::new(new_vk);
        PK_CACHE.insert(circuit_key.clone(), pk_arc.clone());
        VK_CACHE.insert(circuit_key.clone(), vk_arc.clone());
        #[cfg(feature = "debug-logs")]
        eprintln!("‚úÖ SNARK setup completed and cached");
        (pk_arc, vk_arc)
    };
    
    // 2. Prepare SNARK (with cached keys, this is much faster than full setup)
    #[cfg(feature = "debug-logs")]
    eprintln!("üîç Preparing SNARK with cached keys");
    let prep_snark = R1CSSNARK::<E>::prep_prove(&pk, circuit.clone(), true)?; // Enable internal caches
    #[cfg(feature = "debug-logs")]
    eprintln!("‚úÖ SNARK preparation completed (keys were cached)"); 
    
    // 3. Generate proof using prepared SNARK (this is the only non-cacheable step)
    #[cfg(feature = "debug-logs")]
    eprintln!("üîç Generating proof with cached setup and preparation");
    
    #[cfg(feature = "debug-logs")]
    {
        let public_values_len = circuit.public_values().unwrap().len();
        let witness_len = circuit.wit.z_digits.len();
        eprintln!("üìä Proof dimensions: {} public values, {} witness elements", 
                 public_values_len, witness_len);
    }
    
    let snark_proof = R1CSSNARK::<E>::prove(&pk, circuit, &prep_snark, false)
        .map_err(|e| {
            // Only show detailed error analysis in debug mode
            #[cfg(feature = "debug-logs")]
            {
                eprintln!("‚ùå SNARK proving failed: {:?}", e);
                let err_str = format!("{:?}", e);
                if err_str.contains("combine_blinds") {
                    eprintln!("üîç Hash-MLE commitment error - check segment structure");
                } else if err_str.contains("power of two") {
                    eprintln!("üîç Power-of-2 constraint violation");
                }
            }
            e
        })?;
        
    #[cfg(feature = "debug-logs")]
    eprintln!("‚úÖ SNARK proof generated successfully with cached keys!");
    
    // 4. Do NOT self-verify here.
    // Some tests intentionally create inconsistent instances/witnesses to check tamper detection
    // via public IO changes. They expect proving to succeed and verification to fail externally.
    // Instead, deterministically reconstruct the circuit's public values for binding/debugging.
    let public_outputs = {
        let circuit_for_publics = MeCircuit::new(me.clone(), wit.clone(), None, me.header_digest);
        circuit_for_publics
            .public_values()
            .map_err(|e| SpartanError::SynthesisError { reason: format!("public_values() failed: {e}") })?
    };
    
    // 5. Serialize proof
    let proof_bytes = bincode::serialize(&snark_proof)
        .map_err(|e| SpartanError::InternalError { reason: format!("Proof serialization failed: {e}") })?;
    
    // Note: VK doesn't implement Clone, so we need to return the Arc directly
    // The caller will need to handle the Arc wrapper
    Ok((proof_bytes, public_outputs, vk))
}

/// Clear the SNARK caches (useful for testing or memory management)
pub fn clear_snark_caches() {
    PK_CACHE.clear();
    VK_CACHE.clear();
    #[cfg(feature = "debug-logs")]
    eprintln!("üßπ Cleared SNARK key caches");
}

/// Get cache statistics (useful for monitoring)
pub fn get_cache_stats() -> (usize, usize) {
    (PK_CACHE.len(), VK_CACHE.len())
}

// Comprehensive tests added below for circuit fingerprint collision detection

/// Optional helper if you want a structured verify in tests
pub fn verify_me_snark(
    proof_bytes: &[u8],
    vk: &SpartanVerifierKey<E>,
) -> Result<bool, SpartanError> {
    let proof: R1CSSNARK<E> = bincode::deserialize(proof_bytes)
        .map_err(|e| SpartanError::InternalError { reason: format!("bincode(proof) failed: {e}") })?;
    // Spartan2 returns Ok(public_values) on success; we only need success/failure here
    let _ = proof.verify(vk)?;
    Ok(true)
}
