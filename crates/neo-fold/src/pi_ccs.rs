//! Œ†_CCS: Sum-check reduction over extension field K
//!
//! This is the single sum-check used throughout Neo protocol.
//! Proves: Œ£_{u‚àà{0,1}^‚Ñì} (Œ£_i Œ±_i ¬∑ f_i(u)) ¬∑ œá_r(u) = 0

#![allow(non_snake_case)] // Allow mathematical notation like X, T, B

use crate::transcript::{FoldTranscript, Domain};
use crate::error::PiCcsError;
use neo_ccs::{CcsStructure, McsInstance, McsWitness, MeInstance, Mat, MatRef, SparsePoly};
use neo_ajtai::Commitment as Cmt;
use neo_math::{F, K};
use p3_field::{PrimeCharacteristicRing, Field, PrimeField64};
use rayon::prelude::*;
use p3_goldilocks::{Goldilocks, Poseidon2Goldilocks};
use p3_symmetric::Permutation;
use crate::sumcheck::{RoundOracle, run_sumcheck, run_sumcheck_skip_eval_at_one, verify_sumcheck_rounds, SumcheckOutput};

#[allow(dead_code)]
fn format_ext(x: K) -> String { format!("{:?}", x) }

/// Œ†_CCS proof containing the single sum-check over K
#[derive(Debug, Clone)]
pub struct PiCcsProof {
    /// Sum-check protocol rounds (univariate polynomials as coefficients)
    pub sumcheck_rounds: Vec<Vec<K>>,
    /// Extension policy binding digest  
    pub header_digest: [u8; 32],
}

// ===== CSR Sparse Matrix Operations =====

/// Minimal CSR (Compressed Sparse Row) format for sparse matrix operations
/// This enables O(nnz) operations instead of O(n*m) for our extremely sparse matrices
pub struct Csr<F> {
    pub rows: usize,
    pub cols: usize,
    pub indptr: Vec<usize>,  // len = rows + 1
    pub indices: Vec<usize>, // len = nnz  
    pub data: Vec<F>,        // len = nnz
}

/// Convert dense matrix to CSR format - O(nm) but done once
pub fn to_csr<F: Field + Copy>(m: &Mat<F>, rows: usize, cols: usize) -> Csr<F> {
    let mut indptr = Vec::with_capacity(rows + 1);
    let mut indices = Vec::new();
    let mut data = Vec::new();
    indptr.push(0);
    for r in 0..rows {
        let row = m.row(r);
        for (c, &v) in row.iter().enumerate() {
            if v != F::ZERO {
                indices.push(c);
                data.push(v);
            }
        }
        indptr.push(indices.len());
    }
    Csr { rows, cols, indptr, indices, data }
}

/// Sparse matrix-vector multiply: y = A * x  (CSR SpMV, O(nnz))
fn spmv_csr_ff<F: Field + Send + Sync + Copy>(a: &Csr<F>, x: &[F]) -> Vec<F> {
    let mut y = vec![F::ZERO; a.rows];
    y.par_iter_mut().enumerate().for_each(|(r, yr)| {
        let start = a.indptr[r];
        let end = a.indptr[r + 1];
        let mut acc = F::ZERO;
        for k in start..end {
            let c = a.indices[k];
            acc += a.data[k] * x[c];
        }
        *yr = acc;
    });
    y
}

// (dense œá_r variant removed; tests compute the dense check inline to avoid dead code)

// ===== Streaming/half-table equality-weighted CSR transpose multiply =====

/// Row weight provider: returns œá_r(row) or an equivalent row weight.
pub trait RowWeight: Sync {
    fn w(&self, row: usize) -> K;
}

/// Half-table implementation of œá_r row weights to avoid materializing the full tensor.
pub struct HalfTableEq {
    lo: Vec<K>,
    hi: Vec<K>,
    split: usize,
}

impl HalfTableEq {
    pub fn new(r: &[K]) -> Self {
        let ell = r.len();
        let split = ell / 2; // lower split bits in lo, higher in hi
        let lo_len = 1usize << split;
        let hi_len = 1usize << (ell - split);

        // Precompute factors (1-r_i, r_i)
        let mut one_minus = Vec::with_capacity(ell);
        for &ri in r { one_minus.push(K::ONE - ri); }

        // Build lo table
        let mut lo = vec![K::ONE; lo_len];
        for mask in 0..lo_len {
            let mut acc = K::ONE;
            let mut m = mask;
            for i in 0..split {
                let bit = m & 1;
                acc *= if bit == 0 { one_minus[i] } else { r[i] };
                m >>= 1;
            }
            lo[mask] = acc;
        }

        // Build hi table
        let mut hi = vec![K::ONE; hi_len];
        for mask in 0..hi_len {
            let mut acc = K::ONE;
            let mut m = mask;
            for j in 0..(ell - split) {
                let idx = split + j;
                let bit = m & 1;
                acc *= if bit == 0 { one_minus[idx] } else { r[idx] };
                m >>= 1;
            }
            hi[mask] = acc;
        }

        Self { lo, hi, split }
    }
}

impl RowWeight for HalfTableEq {
    #[inline]
    fn w(&self, row: usize) -> K {
        let lo_mask = (1usize << self.split) - 1;
        let lo_idx = row & lo_mask;
        let hi_idx = row >> self.split;
        self.lo[lo_idx] * self.hi[hi_idx]
    }
}

/// Weighted version of CSR transpose multiply: v = A^T * w, where w(row) is provided on-the-fly.
pub fn spmv_csr_t_weighted_fk<W: RowWeight + Sync>(a: &Csr<F>, w: &W) -> Vec<K> {
    let mut v = vec![K::ZERO; a.cols];
    let chunk = a.rows / rayon::current_num_threads().max(1) + 1;
    let partials: Vec<Vec<(usize, K)>> = (0..a.rows)
        .into_par_iter()
        .with_min_len(chunk)
        .map(|r| {
            let mut loc = Vec::with_capacity(a.indptr[r + 1] - a.indptr[r]);
            let wr = w.w(r);
            for k in a.indptr[r]..a.indptr[r + 1] {
                let c = a.indices[k];
                loc.push((c, K::from(a.data[k]) * wr));
            }
            loc
        })
        .collect();
    for loc in partials { for (c, val) in loc { v[c] += val; } }
    v
}


// ===== Matrix Digest Optimization =====

/// Compute canonical sparse digest of CCS matrices to avoid absorbing 500M+ field elements  
/// Uses ZK-friendly Poseidon2 hash with proper domain separation
fn digest_ccs_matrices<F: Field + PrimeField64>(s: &CcsStructure<F>) -> Vec<Goldilocks> {
    use rand_chacha::{ChaCha8Rng, rand_core::SeedableRng};
    
    // Use fixed seed for deterministic hashing (equivalent to derive_key concept)
    const CCS_DIGEST_SEED: u64 = 0x434353445F4D4154; // "CCSD_MAT" in hex
    let mut rng = ChaCha8Rng::seed_from_u64(CCS_DIGEST_SEED);
    let poseidon2 = Poseidon2Goldilocks::<16>::new_from_rng_128(&mut rng);
    
    // Sponge state for Poseidon2 (width=16)
    let mut state = [Goldilocks::ZERO; 16];
    let mut absorbed = 0;
    
    // PROPER DOMAIN SEPARATION: Absorb context string byte-by-byte (same as transcript pattern)
    const DOMAIN_STRING: &[u8] = b"neo/ccs/matrices/v1"; 
    for &byte in DOMAIN_STRING {
        if absorbed >= 15 { // Leave room for rate limiting
            poseidon2.permute_mut(&mut state);
            absorbed = 0;
        }
        state[absorbed] = Goldilocks::from_u32(byte as u32);
        absorbed += 1;
    }
    
    // Absorb matrix dimensions  
    if absorbed + 3 >= 16 { poseidon2.permute_mut(&mut state); absorbed = 0; }
    state[absorbed] = Goldilocks::from_u64(s.n as u64);
    state[absorbed + 1] = Goldilocks::from_u64(s.m as u64); 
    state[absorbed + 2] = Goldilocks::from_u64(s.t() as u64);
    // Note: absorbed will be reset to 0 in the loop below, so no need to track it here
    
    poseidon2.permute_mut(&mut state);
    
    // Absorb each matrix in sparse format
    for (j, matrix) in s.matrices.iter().enumerate() {
        // Reset and start with matrix index
        absorbed = 0;
        state[absorbed] = Goldilocks::from_u64(j as u64);
        absorbed += 1;
        
        let mat_ref = MatRef::from_mat(matrix);
        
        // Absorb sparse entries in canonical (row, col, val) order
        for row in 0..s.n {
            let row_slice = mat_ref.row(row);
            for (col, &val) in row_slice.iter().enumerate() {
                if val != F::ZERO {
                    if absorbed + 3 > 15 { // Leave room for rate limiting
                        poseidon2.permute_mut(&mut state);
                        absorbed = 0;
                    }
                    
                    // Absorb (row, col, val) as consecutive field elements
                    state[absorbed] = Goldilocks::from_u64(row as u64);
                    state[absorbed + 1] = Goldilocks::from_u64(col as u64);
                    state[absorbed + 2] = Goldilocks::from_u64(val.as_canonical_u64());
                    absorbed += 3;
                }
            }
        }
        
        // Permute after each matrix to ensure proper mixing
        poseidon2.permute_mut(&mut state);
    }
    
    // Return first 4 field elements as digest (128 bits of security)
    state[0..4].to_vec()
}

// ===== MLE Folding DP Helpers =====

/// Pad vector to power of 2 length with zeros
#[inline]
fn pad_to_pow2_k(mut v: Vec<K>, ell: usize) -> Result<Vec<K>, PiCcsError> {
    let want = 1usize << ell;
    if v.len() > want {
        return Err(PiCcsError::SumcheckError(format!(
            "Vector length {} exceeds 2^ell = {} - would silently truncate", 
            v.len(), want
        )));
    }
    v.resize(want, K::ZERO);
    Ok(v)
}

// ===== Blocked Parallel Matrix-Vector Multiplication =====

/// Absorb sparse polynomial definition into transcript for soundness binding.
/// This prevents a malicious prover from using different polynomials with the same matrix structure.
fn absorb_sparse_polynomial(tr: &mut FoldTranscript, f: &SparsePoly<F>) {
    // Absorb polynomial structure
    tr.absorb_bytes(b"neo/ccs/poly");
    tr.absorb_u64(&[f.arity() as u64]);
    tr.absorb_u64(&[f.terms().len() as u64]);
    
    // Absorb each term: coefficient + exponents (sorted for determinism)
    let mut terms: Vec<_> = f.terms().iter().collect();
    terms.sort_by_key(|term| &term.exps); // deterministic ordering
    
    for term in terms {
        tr.absorb_f(&[term.coeff]);
        tr.absorb_u64(&term.exps.iter().map(|&e| e as u64).collect::<Vec<_>>());
    }
}

// Detect whether a CCS structure encodes an R1CS relation: t=3 and f(y)=y0*y1 - y2.
fn is_r1cs_shape(s: &CcsStructure<F>) -> bool {
    // Detect up to a nonzero scalar multiple and variable permutation:
    // f(y0,y1,y2) = Œª¬∑(y_i ¬∑ y_j ‚àí y_k), where {i,j,k} = {0,1,2} and Œª ‚â† 0.
    if s.t() != 3 || s.f.arity() != 3 { return false; }
    use std::collections::BTreeMap;
    let mut map: BTreeMap<[u32; 3], F> = BTreeMap::new();
    for term in s.f.terms() {
        if term.exps.len() != 3 { return false; }
        let mut key = [0u32; 3];
        key.copy_from_slice(&term.exps[..3]);
        *map.entry(key).or_insert(F::ZERO) += term.coeff;
    }
    // Drop zero coefficients
    map.retain(|_, c| *c != F::ZERO);
    if map.len() != 2 { return false; }

    // Identify the product term (two ones) and the single term (one one)
    let mut prod_key = None; let mut prod_coeff = F::ZERO;
    let mut single_key = None; let mut single_coeff = F::ZERO;
    for (k, &c) in &map {
        let wt: u32 = k.iter().copied().sum();
        match wt {
            2 => {
                // Must be exactly [1,1,0] up to permutation
                if k.iter().all(|&e| e <= 1) { prod_key = Some(*k); prod_coeff = c; }
            }
            1 => {
                if k.iter().all(|&e| e <= 1) { single_key = Some(*k); single_coeff = c; }
            }
            _ => return false,
        }
    }
    let (prod, single) = match (prod_key, single_key) { (Some(p), Some(s)) => (p, s), _ => return false };
    // The single 1 must be on the index where the product has a 0 (complement index)
    let zero_idx = prod.iter().position(|&e| e == 0).unwrap();
    if single[zero_idx] != 1 { return false; }
    // Coefficients must be opposite up to a nonzero scalar: single = -prod
    if prod_coeff == F::ZERO { return false; }
    single_coeff == -prod_coeff
}

// ===== Local MLE partial structures and Sum-check round oracles =====

// Generic CCS partials: one shrinking vector per matrix M_j z
struct MlePartials { s_per_j: Vec<Vec<K>> }
// R1CS residual partials: one shrinking vector over row-wise residuals
#[allow(dead_code)]
struct MleResiduals { s: Vec<K> }

struct GenericCcsOracle<'a> {
    s: &'a CcsStructure<F>,
    alphas: Vec<K>,
    partials_per_inst: Vec<MlePartials>,
    // Small-value fast path for round 0: access original F-vectors without lifting
    mz_f_per_inst: Vec<Vec<&'a [F]>>,
    ell: usize,
    d_sc: usize,
    first_round_done: bool,
}
impl<'a> RoundOracle for GenericCcsOracle<'a> {
    fn num_rounds(&self) -> usize { self.ell }
    fn degree_bound(&self) -> usize { self.d_sc }
    fn evals_at(&mut self, sample_xs: &[K]) -> Vec<K> {
        let t = self.s.t();
        let mut sample_ys = vec![K::ZERO; sample_xs.len()];
        if !self.first_round_done {
            // Round-0 small-value path: accumulate in F, then lift once per sample X
            for (inst_idx, mz_f) in self.mz_f_per_inst.iter().enumerate() {
                let mut a_f = vec![F::ZERO; t];
                let mut d_f = vec![F::ZERO; t];
                let n_pad = 1usize << self.ell;
                let half = n_pad >> 1;
                for j in 0..t {
                    let v_f = mz_f[j];
                    let mut aj = F::ZERO;
                    let mut dj = F::ZERO;
                    for k in 0..half {
                        let idx_e = 2*k;
                        let idx_o = idx_e + 1;
                        let e = if idx_e < v_f.len() { v_f[idx_e] } else { F::ZERO };
                        let o = if idx_o < v_f.len() { v_f[idx_o] } else { F::ZERO };
                        aj += e;
                        dj += o - e;
                    }
                    a_f[j] = aj; d_f[j] = dj;
                }
                let alpha = self.alphas[inst_idx];
                let mut y_buf = vec![K::ZERO; t];
                for (sx, &X) in sample_xs.iter().enumerate() {
                    for j in 0..t { y_buf[j] = K::from(a_f[j]) + K::from(d_f[j]) * X; }
                    let f_eval = self.s.f.eval_in_ext::<K>(&y_buf);
                    sample_ys[sx] += alpha * f_eval;
                }
            }
            self.first_round_done = true;
        } else {
            for (inst_idx, partials) in self.partials_per_inst.iter().enumerate() {
                let mut a = vec![K::ZERO; t];
                let mut delta = vec![K::ZERO; t];
                for j in 0..t {
                    let v = &partials.s_per_j[j];
                    debug_assert!(v.len().is_power_of_two() && v.len() >= 2);
                    let half = v.len() >> 1;
                    let (mut aj, mut dj) = (K::ZERO, K::ZERO);
                    for k in 0..half {
                        let e = v[2*k];
                        let o = v[2*k + 1];
                        aj += e;
                        dj += o - e;
                    }
                    a[j] = aj; delta[j] = dj;
                }
                let alpha = self.alphas[inst_idx];
                let mut y_buf = vec![K::ZERO; t];
                for (sx, &X) in sample_xs.iter().enumerate() {
                    for j in 0..t { y_buf[j] = a[j] + delta[j] * X; }
                    let f_eval = self.s.f.eval_in_ext::<K>(&y_buf);
                    sample_ys[sx] += alpha * f_eval;
                }
            }
        }
        sample_ys
    }
    fn fold(&mut self, r_i: K) {
        for partials in self.partials_per_inst.iter_mut() {
            for v in &mut partials.s_per_j {
                let n2 = v.len() >> 1;
                for k in 0..n2 {
                    let a0 = v[2*k]; let b0 = v[2*k + 1];
                    v[k] = (K::ONE - r_i) * a0 + r_i * b0;
                }
                v.truncate(n2);
            }
        }
    }
}

// Legacy residual oracle removed (replaced by eq-weighted oracle)

// === Eq-weighted R1CS oracle: binds product via equality MLE ===
struct R1csEqOracle {
    alphas: Vec<K>,
    a_per_inst: Vec<Vec<K>>, // shrinking tables (length halves each round)
    b_per_inst: Vec<Vec<K>>, 
    c_per_inst: Vec<Vec<K>>, 
    w: Vec<K>,               // eq-binding vector
    eq_prefix: K,            // ‚àè_{j<i} eq(w_j, r_j)
    ell: usize,
    round: usize,
}

impl R1csEqOracle {
    #[inline]
    fn eq_lin(wi: K, x: K) -> K { (K::ONE - wi) * (K::ONE - x) + wi * x }
}

impl RoundOracle for R1csEqOracle {
    fn num_rounds(&self) -> usize { self.ell }
    fn degree_bound(&self) -> usize { 3 }
    fn evals_at(&mut self, xs: &[K]) -> Vec<K> {
        debug_assert!(self.a_per_inst.iter().all(|a| a.len().is_power_of_two()));
        debug_assert!(self.b_per_inst.iter().all(|a| a.len().is_power_of_two()));
        debug_assert!(self.c_per_inst.iter().all(|a| a.len().is_power_of_two()));
        let i = self.round;
        let wi = self.w[i];
        let pair_cnt = self.a_per_inst[0].len() >> 1;
        // Build suffix weights eq(w_{>i}, ¬∑) of length 2^{ell-i-1}
        let rem = self.ell - (i + 1);
        let mut suffix = vec![K::ONE; pair_cnt];
        for mask in 0..pair_cnt {
            let mut acc = K::ONE;
            let mut m = mask;
            for j in 0..rem {
                let wj = self.w[i + 1 + j];
                acc *= if (m & 1) == 0 { K::ONE - wj } else { wj };
                m >>= 1;
            }
            suffix[mask] = acc;
        }

        let mut ys = vec![K::ZERO; xs.len()];
        for inst_idx in 0..self.a_per_inst.len() {
            let alpha = self.alphas[inst_idx];
            let a = &self.a_per_inst[inst_idx];
            let b = &self.b_per_inst[inst_idx];
            let c = &self.c_per_inst[inst_idx];
            for k in 0..pair_cnt {
                let e = 2*k;
                let o = e + 1;
                let a0 = a[e]; let da = a[o] - a[e];
                let b0 = b[e]; let db = b[o] - b[e];
                let c0 = c[e]; let dc = c[o] - c[e];
                let w_suf = suffix[k];
                for (t, &x) in xs.iter().enumerate() {
                    let ax = a0 + da * x;
                    let bx = b0 + db * x;
                    let cx = c0 + dc * x;
                    let res = ax * bx - cx; // deg 2
                    let eqix = Self::eq_lin(wi, x); // deg 1
                    ys[t] += alpha * self.eq_prefix * eqix * w_suf * res;
                }
            }
        }
        ys
    }
    fn fold(&mut self, r_i: K) {
        // Shrink tables
        for a in &mut self.a_per_inst {
            let n2 = a.len() >> 1; for k in 0..n2 { a[k] = (K::ONE - r_i) * a[2*k] + r_i * a[2*k + 1]; } a.truncate(n2);
        }
        for b in &mut self.b_per_inst {
            let n2 = b.len() >> 1; for k in 0..n2 { b[k] = (K::ONE - r_i) * b[2*k] + r_i * b[2*k + 1]; } b.truncate(n2);
        }
        for c in &mut self.c_per_inst {
            let n2 = c.len() >> 1; for k in 0..n2 { c[k] = (K::ONE - r_i) * c[2*k] + r_i * c[2*k + 1]; } c.truncate(n2);
        }
        // Extend eq prefix
        let wi = self.w[self.round];
        self.eq_prefix *= Self::eq_lin(wi, r_i);
        self.round += 1;
    }
}

/// Batching coefficients for the composed polynomial Q
/// 
/// In v1: Only CCS constraints are included in Q(u).
/// Range and evaluation tie constraints are handled outside the sum-check.
#[derive(Debug, Clone)]
struct BatchingCoeffs {
    /// Œ± coefficients for CCS constraints f(Mz)  
    alphas: Vec<K>,
}

/// Evaluate range/decomposition constraint polynomials NC_i(z,Z) at point u.
/// These assert: Z = Decomp_b(z) and ||Z||_‚àû < b
/// 
/// NOTE: For honest instances where Z == Decomp_b(z) and ||Z||_‚àû < b, 
///       this MUST return zero to make the composed polynomial Q sum to zero.
pub fn eval_range_decomp_constraints(
    z: &[F],
    Z: &neo_ccs::Mat<F>,
    _u: &[K],                  // not used: constraints are independent of u
    params: &neo_params::NeoParams,
) -> K {
    // REAL CONSTRAINT EVALUATION (degree 0 in u)
    // Enforces two facts:
    // 1. Decomposition correctness: z[c] = Œ£_{i=0}^{d-1} b^i * Z[i,c] 
    // 2. Digit range (balanced): R_b(x) = x * ‚àè_{t=1}^{b-1} (x-t)(x+t) = 0
    
    let d = Z.rows();
    let m = Z.cols();

    // Sanity: shapes
    if z.len() != m {
        // Treat shape mismatch as a hard violation: contribute a non-zero sentinel.
        return K::from(F::ONE);
    }

    // Precompute base powers in F for recomposition
    let b_f = F::from_u64(params.b as u64);
    let mut pow_b = vec![F::ONE; d];
    for i in 1..d { 
        pow_b[i] = pow_b[i - 1] * b_f; 
    }

    // === (A) Decomposition correctness residual: sum of squares in K ===
    let mut decomp_residual = K::ZERO;
    for c in 0..m {
        // z_rec = Œ£_{i=0..d-1} (b^i * Z[i,c])
        let mut z_rec_f = F::ZERO;
        for i in 0..d {
            z_rec_f += Z[(i, c)] * pow_b[i];
        }
        // Residual (in K): (z_rec - z[c])^2
        let diff_k = K::from(z_rec_f) - K::from(z[c]);
        decomp_residual += diff_k * diff_k;
    }

    // === (B) Range residual: R_b(x) = x * ‚àè_{t=1}^{b-1} (x - t)(x + t) for every digit ===
    // Works for the "Balanced" digit set in your code path; if you ever switch styles,
    // generalize this polynomial accordingly.
    let mut range_residual = K::ZERO;

    // Precompute constants in F for 1..(b-1)
    let mut t_vals = Vec::with_capacity((params.b - 1) as usize);
    for t in 1..params.b {
        t_vals.push(F::from_u64(t as u64));
    }

    for c in 0..m {
        for i in 0..d {
            let x = Z[(i, c)];            // digit in F
            // Build R_b(x) in K
            let mut rbx = K::from(x);     // starts with the leading x factor
            // Multiply (x - t)(x + t) over t=1..b-1
            for &t in &t_vals {
                rbx *= K::from(x) - K::from(t);   // (x - t)
                rbx *= K::from(x) + K::from(t);   // (x + t)
            }
            range_residual += rbx;
        }
    }

    decomp_residual + range_residual
}

/// Evaluate tie constraint polynomials ‚ü®M_j^T œá_u, Z‚ü© - y_j at point u.
/// These assert that the y_j values are consistent with Z and the random point.
/// 
/// CRITICAL: This must be implemented correctly for soundness!
/// The sum-check terminal verification depends on this being accurate.
pub fn eval_tie_constraints(
    s: &CcsStructure<F>,
    Z: &neo_ccs::Mat<F>,
    claimed_y: &[Vec<K>], // y_j entries as produced in ME (length t, each length d)
    u: &[K],
) -> K {
    // REAL MULTILINEAR EXTENSION EVALUATION
    // Implements: Œ£_j Œ£_œÅ (‚ü®Z_œÅ,*, M_j^T œá_u‚ü© - y_{j,œÅ})
    
    // œá_u ‚àà K^n
    let chi_u = neo_ccs::utils::tensor_point::<K>(u);

    let d = Z.rows();       // Ajtai dimension
    let m = Z.cols();       // number of columns in Z (== s.m)

    debug_assert_eq!(m, s.m, "Z.cols() must equal s.m");
    
    // If claimed_y is missing or has wrong shape, we conservatively treat it as zero.
    // (This will force the prover's Q(u) to carry the full ‚ü®Z, M_j^T œá_u‚ü© mass, which
    // then must cancel at r when the real y_j are used.)
    let safe_y = |j: usize, rho: usize| -> K {
        if j < claimed_y.len() && rho < claimed_y[j].len() {
            claimed_y[j][rho]
        } else {
            K::ZERO
        }
    };

    let mut total = K::ZERO;

    // For each matrix M_j, build v_j(u) = M_j^T œá_u ‚àà K^m, then compute Z * v_j(u) ‚àà K^d
    for (j, mj) in s.matrices.iter().enumerate() {
        // v_j[c] = Œ£_{row=0..n-1} M_j[row,c] * œá_u[row]
        let mut vj = vec![K::ZERO; s.m];
        for row in 0..s.n {
            let coeff = chi_u[row];
            let mj_row = mj.row(row);
            for c in 0..s.m {
                vj[c] += K::from(mj_row[c]) * coeff;
            }
        }
        // lhs = Z * v_j(u) as a length-d K-vector
        let z_ref = neo_ccs::MatRef::from_mat(Z);
        let lhs = neo_ccs::utils::mat_vec_mul_fk::<F, K>(z_ref.data, z_ref.rows, z_ref.cols, &vj);

        // Accumulate the residual (lhs - y_j)
        for rho in 0..d {
            total += lhs[rho] - safe_y(j, rho);
        }
    }

    total
}

/// Prove Œ†_CCS: CCS instances satisfy constraints via sum-check
/// 
/// Input: k+1 CCS instances, outputs k ME instances + proof
pub fn pi_ccs_prove<L: neo_ccs::traits::SModuleHomomorphism<F, Cmt>>(
    tr: &mut FoldTranscript,
    params: &neo_params::NeoParams,
    s: &CcsStructure<F>,
    mcs_list: &[McsInstance<Cmt, F>],
    witnesses: &[McsWitness<F>],
    l: &L, // we need L to check c = L(Z) and to compute X = L_x(Z)
) -> Result<(Vec<MeInstance<Cmt, F, K>>, PiCcsProof), PiCcsError> {
    tr.domain(Domain::CCS);

    // --- Input & policy checks ---
    if mcs_list.is_empty() || mcs_list.len() != witnesses.len() {
        return Err(PiCcsError::InvalidInput("empty or mismatched inputs".into()));
    }
    // >>> CHANGE #1: allow arbitrary n; compute ‚Ñì from next power of two
    if s.n == 0 {
        return Err(PiCcsError::InvalidInput("n=0 not allowed".into()));
    }
    let n_pad = s.n.next_power_of_two();
    let ell = n_pad.trailing_zeros() as usize;
    let d_sc = s.max_degree() as usize;

    let ext = params.extension_check(ell as u32, d_sc as u32)
        .map_err(|e| PiCcsError::ExtensionPolicyFailed(format!("Extension policy validation failed: {}", e)))?;
    // Enforce strict security policy: slack_bits must be non-negative
    // This ensures we meet or exceed the target lambda-bit security level
    if ext.slack_bits < 0 {
        return Err(PiCcsError::ExtensionPolicyFailed(format!(
            "Insufficient security slack: {} bits (need ‚â• 0 for target {}-bit security)", 
            ext.slack_bits, params.lambda
        )));
    }
    tr.absorb_ccs_header(64, ext.s_supported, params.lambda, ell as u32, d_sc as u32, ext.slack_bits);

    // MAJOR OPTIMIZATION: Convert to CSR sparse format once for all operations
    // This enables O(nnz) operations instead of O(n*m) for our extremely sparse matrices  
    let csr_start = std::time::Instant::now();
    let mats_csr: Vec<Csr<F>> = s.matrices.iter().map(|m| to_csr::<F>(m, s.n, s.m)).collect();
    let total_nnz: usize = mats_csr.iter().map(|c| c.data.len()).sum();
    println!("üî• [NUCLEAR] CSR conversion completed: {:.2}ms ({} matrices, {} nnz total, {:.4}% density)",
             csr_start.elapsed().as_secs_f64() * 1000.0, 
             mats_csr.len(), total_nnz, 
             (total_nnz as f64) / (s.n * s.m * s.matrices.len()) as f64 * 100.0);

    // --- Prepare per-instance data and check c=L(Z) ---
    // Also build z = x||w and cache M_j z over F for each instance.
    struct Inst<'a> {
        Z: &'a Mat<F>, 
        m_in: usize, 
        mz: Vec<Vec<F>>,
        c: Cmt,
    }
    let mut insts: Vec<Inst> = Vec::with_capacity(mcs_list.len());
    let instance_prep_start = std::time::Instant::now();
    for (inst_idx, (inst, wit)) in mcs_list.iter().zip(witnesses.iter()).enumerate() {
        let z_check_start = std::time::Instant::now();
        let z = neo_ccs::relations::check_mcs_opening(l, inst, wit)
            .map_err(|e| PiCcsError::InvalidInput(format!("MCS opening failed: {e}")))?;
        println!("üîß [INSTANCE {}] MCS opening check: {:.2}ms", inst_idx, 
                 z_check_start.elapsed().as_secs_f64() * 1000.0);
        // SECURITY: Ensure z matches CCS width to prevent OOB during M*z
        if z.len() != s.m {
            return Err(PiCcsError::InvalidInput(format!(
                "SECURITY: z length {} does not match CCS column count {} (malformed instance)",
                z.len(), s.m
            )));
        }
        
        // === CRITICAL SECURITY CHECK: Z == Decomp_b(z) ===
        // This prevents prover from using satisfying z for CCS but different Z for commitment
        let decomp_start = std::time::Instant::now();
        let Z_expected_col_major = neo_ajtai::decomp_b(&z, params.b, neo_math::D, neo_ajtai::DecompStyle::Balanced);
        println!("üîß [INSTANCE {}] Decomp_b: {:.2}ms", inst_idx, 
                 decomp_start.elapsed().as_secs_f64() * 1000.0);
        
        let range_check_start = std::time::Instant::now();
        neo_ajtai::assert_range_b(&Z_expected_col_major, params.b)
            .map_err(|e| PiCcsError::InvalidInput(format!("Range check failed on expected Z: {e}")))?;
        println!("üîß [INSTANCE {}] Range check: {:.2}ms", inst_idx, 
                 range_check_start.elapsed().as_secs_f64() * 1000.0);
        
        // Convert Z_expected from column-major to row-major format to match wit.Z
        let format_conv_start = std::time::Instant::now();
        let d = neo_math::D;
        let m = Z_expected_col_major.len() / d;
        let mut Z_expected_row_major = vec![neo_math::F::ZERO; d * m];
        for col in 0..m {
            for row in 0..d {
                let col_major_idx = col * d + row;
                let row_major_idx = row * m + col;
                Z_expected_row_major[row_major_idx] = Z_expected_col_major[col_major_idx];
            }
        }
        println!("üîß [INSTANCE {}] Format conversion: {:.2}ms", inst_idx, 
                 format_conv_start.elapsed().as_secs_f64() * 1000.0);
        
        // Compare Z with expected decomposition (both in row-major format)
        let z_compare_start = std::time::Instant::now();
        if wit.Z.as_slice() != Z_expected_row_major.as_slice() {
            return Err(PiCcsError::InvalidInput("SECURITY: Z != Decomp_b(z) - prover using inconsistent z and Z".into()));
        }
        println!("üîß [INSTANCE {}] Z comparison: {:.2}ms", inst_idx, 
                 z_compare_start.elapsed().as_secs_f64() * 1000.0);
        // MAJOR OPTIMIZATION: Use CSR sparse matrix-vector multiply - O(nnz) instead of O(n*m)!
        let mz_start = std::time::Instant::now();
        let mz: Vec<Vec<F>> = mats_csr.par_iter().map(|csr| 
            spmv_csr_ff::<F>(csr, &z)
        ).collect();
        println!("üí• [TIMING] CSR M_j z computation: {:.2}ms (nnz={}, vs {}M dense elements - {}x reduction)", 
                 mz_start.elapsed().as_secs_f64() * 1000.0, total_nnz, 
                 (s.n * s.m * s.matrices.len()) / 1_000_000,
                 (s.n * s.m * s.matrices.len()) / total_nnz.max(1));
        insts.push(Inst{ Z: &wit.Z, m_in: inst.m_in, mz, c: inst.c.clone() });
    }
    println!("üîß [TIMING] Instance preparation total: {:.2}ms ({} instances)", 
             instance_prep_start.elapsed().as_secs_f64() * 1000.0, insts.len());

    // --- SECURITY: Absorb instance data BEFORE sampling challenges to prevent malleability ---
    let transcript_start = std::time::Instant::now();
    tr.absorb_bytes(b"neo/ccs/instances");
    tr.absorb_u64(&[s.n as u64, s.m as u64, s.t() as u64]);
    // OPTIMIZATION: Absorb compact ZK-friendly digest instead of 500M+ field elements 
    // This reduces transcript absorption from ~51s to microseconds using Poseidon2
    let matrix_digest = digest_ccs_matrices(s);
    for &digest_elem in &matrix_digest {
        tr.absorb_f(&[F::from_u64(digest_elem.as_canonical_u64())]);
    }
    // CRITICAL: Absorb polynomial definition to prevent malicious polynomial substitution
    absorb_sparse_polynomial(tr, &s.f);
    
    // Absorb all instance data (commitment, public inputs, witness structure)
    for inst in mcs_list.iter() {
        // Absorb instance data that affects soundness
        tr.absorb_f(&inst.x);
        tr.absorb_u64(&[inst.m_in as u64]);
        // CRITICAL: Absorb commitment to prevent cross-instance attacks
        tr.absorb_f(&inst.c.data);
    }
    println!("üîß [TIMING] Transcript absorption: {:.2}ms", 
             transcript_start.elapsed().as_secs_f64() * 1000.0);

    // --- Generate eq-binding vector and batching coefficients for composed polynomial Q ---
    let batching_start = std::time::Instant::now();
    // Eq-binding vector w sampled before batching (used only when t>=3)
    tr.absorb_bytes(b"neo/ccs/eq");
    let w_eq: Vec<K> = (0..ell).map(|_| tr.challenge_k()).collect();
    tr.absorb_bytes(b"neo/ccs/batch");
    
    // Œ± coefficients for CCS constraints (one per instance)
    let alphas: Vec<K> = (0..insts.len()).map(|_| tr.challenge_k()).collect();
    
    let batch_coeffs = BatchingCoeffs { alphas };
    println!("üîß [TIMING] Batching coefficients: {:.2}ms", 
             batching_start.elapsed().as_secs_f64() * 1000.0);

    // --- Run sum-check rounds over composed polynomial Q (degree ‚â§ d_sc) ---
    println!("üîç Sum-check starting: {} instances, {} rounds", insts.len(), ell);
    let sample_xs_generic: Vec<K> = (0..=d_sc as u64).map(|u| K::from(F::from_u64(u))).collect();
    let sample_xs_r1cs: Vec<K> = (0..=3u64).map(|u| K::from(F::from_u64(u))).collect();
    // Enable R1CS eq-binding terminal check whenever the CCS is R1CS-shaped.
    let use_r1cs = is_r1cs_shape(s);
    let use_generic_ccs = !use_r1cs;

    // Build partial states (invariant across the engine)
    let mle_start = std::time::Instant::now();
    let partials_per_inst_opt: Option<Vec<MlePartials>>;
    if use_generic_ccs {
        let partials: Result<Vec<MlePartials>, PiCcsError> = insts.par_iter().map(|inst| {
            let mut s_per_j = Vec::with_capacity(s.t());
            for j in 0..s.t() {
                let w_k: Vec<K> = inst.mz[j].iter().map(|&x| K::from(x)).collect();
                let w_k = pad_to_pow2_k(w_k, ell)?;
                s_per_j.push(w_k);
            }
            Ok(MlePartials { s_per_j })
        }).collect();
        partials_per_inst_opt = Some(partials?);
    } else {
        // R1CS eq-weighted oracle uses A,B,C shrinking tables instead of residuals
        partials_per_inst_opt = None;
    }
    println!("üîß [TIMING] MLE partials setup: {:.2}ms",
             mle_start.elapsed().as_secs_f64() * 1000.0);

    // Drive rounds with the generic engine
    let initial_sum = K::ZERO;
    let SumcheckOutput { rounds, challenges: r, final_sum: _running_sum } = if use_generic_ccs {
        let mut oracle = GenericCcsOracle {
            s, alphas: batch_coeffs.alphas.clone(),
            partials_per_inst: partials_per_inst_opt.unwrap(),
            mz_f_per_inst: insts.iter().map(|inst| inst.mz.iter().map(|v| v.as_slice()).collect()).collect(),
            ell, d_sc,
            first_round_done: false,
        };
        if d_sc >= 1 { run_sumcheck_skip_eval_at_one(tr, &mut oracle, initial_sum, &sample_xs_generic)? }
        else { run_sumcheck(tr, &mut oracle, initial_sum, &sample_xs_generic)? }
    } else {
        // Build A,B,C shrinking tables per instance
        let mut a_per_inst = Vec::with_capacity(insts.len());
        let mut b_per_inst = Vec::with_capacity(insts.len());
        let mut c_per_inst = Vec::with_capacity(insts.len());
        for inst in &insts {
            let mut a_vk: Vec<K> = inst.mz[0].iter().map(|&x| K::from(x)).collect();
            let mut b_vk: Vec<K> = inst.mz[1].iter().map(|&x| K::from(x)).collect();
            let mut c_vk: Vec<K> = inst.mz[2].iter().map(|&x| K::from(x)).collect();
            a_vk = pad_to_pow2_k(a_vk, ell)?;
            b_vk = pad_to_pow2_k(b_vk, ell)?;
            c_vk = pad_to_pow2_k(c_vk, ell)?;
            a_per_inst.push(a_vk);
            b_per_inst.push(b_vk);
            c_per_inst.push(c_vk);
        }
        let mut oracle = R1csEqOracle {
            alphas: batch_coeffs.alphas.clone(),
            a_per_inst,
            b_per_inst,
            c_per_inst,
            w: w_eq.clone(),
            eq_prefix: K::ONE,
            ell,
            round: 0,
        };
        run_sumcheck_skip_eval_at_one(tr, &mut oracle, initial_sum, &sample_xs_r1cs)?
    };

    println!("üîß [TIMING] Sum-check rounds complete ({} rounds)", ell);

    // Compute M_j^T * œá_r using streaming/half-table weights (no full œá_r materialization)
    println!("üöÄ [OPTIMIZATION] Computing M_j^T * œá_r with half-table weights...");
    let transpose_once_start = std::time::Instant::now();
    let w = HalfTableEq::new(&r);
    let vjs: Vec<Vec<K>> = mats_csr.par_iter()
        .map(|csr| spmv_csr_t_weighted_fk::<_>(csr, &w))
        .collect();
    println!("üí• [OPTIMIZATION] Weighted CSR M_j^T * œá_r computed: {:.2}ms (nnz={})",
             transpose_once_start.elapsed().as_secs_f64() * 1000.0, total_nnz);

    // --- Build ME instances (one per input) ---
    let me_start = std::time::Instant::now();
    
    // CRITICAL SECURITY FIX: Generate fold_digest from final transcript state
    // This binds the ME instances to the exact folding proof and prevents re-binding attacks
    let fold_digest = tr.state_digest();
    
    let mut out_me = Vec::with_capacity(insts.len());
    for (inst_idx, inst) in insts.iter().enumerate() {
        // X = L_x(Z)
        let X = l.project_x(inst.Z, inst.m_in);
        
        // OPTIMIZATION: Use precomputed v_j vectors and MLE fold results  
        let mut y = Vec::with_capacity(s.t());
        // Use final MLE fold results for Y_j(r) (already computed above) ‚Äî we do not maintain per-j partials
        // here, so reconstruct Y_j(r) via sparse transpose multiply below.
        // We still populate y_scalars with the CORRECT scalars: ‚ü®(M_j z), œá_r‚ü© derived below.
        
        // Use precomputed v_j = M_j^T * œá_r vectors (no more expensive recomputation per instance!)
        let z_operations_start = std::time::Instant::now();
        for (_j, vj) in vjs.iter().enumerate() {
            let z_ref = neo_ccs::MatRef::from_mat(inst.Z);
            let yj = neo_ccs::utils::mat_vec_mul_fk::<F,K>(z_ref.data, z_ref.rows, z_ref.cols, vj);
            y.push(yj);
        }
        println!("üöÄ [OPTIMIZATION] Used precomputed v_j vectors - only Z * v_j needed: {:.2}ms", 
                 z_operations_start.elapsed().as_secs_f64() * 1000.0);
        
        // Compute the CORRECT Y_j(r) scalars: ‚ü®(M_j z), œá_r‚ü© using streaming weights
        let y_scalars: Vec<K> = (0..s.t()).map(|j| {
            let mut acc = K::ZERO;
            for i in 0..s.n {
                acc += K::from(insts[inst_idx].mz[j][i]) * w.w(i);
            }
            acc
        }).collect();

        out_me.push(MeInstance{ 
            c_step_coords: vec![], // Pattern B: Populated by IVC layer, not folding
            u_offset: 0,  // Pattern B: Unused (computed deterministically from witness structure)
            u_len: 0,     // Pattern B: Unused (computed deterministically from witness structure)
            c: inst.c.clone(), 
            X, 
            r: r.clone(), 
            y, 
            y_scalars, // SECURITY: Correct scalars for terminal check
            m_in: inst.m_in,
            fold_digest, // Bind to transcript
        });
    }

    println!("üîß [TIMING] ME instance building: {:.2}ms", 
             me_start.elapsed().as_secs_f64() * 1000.0);

    let proof = PiCcsProof { sumcheck_rounds: rounds, header_digest: fold_digest };
    Ok((out_me, proof))
}

/// Verify Œ†_CCS: Check sum-check rounds AND the critical final claim Q(r) = 0
pub fn pi_ccs_verify(
    tr: &mut FoldTranscript,
    params: &neo_params::NeoParams,
    s: &CcsStructure<F>,
    mcs_list: &[McsInstance<Cmt, F>], // Now used for final Q(r) check
    out_me: &[MeInstance<Cmt, F, K>],
    proof: &PiCcsProof,
) -> Result<bool, PiCcsError> {
    tr.domain(Domain::CCS);
    // >>> CHANGE #2: allow arbitrary n; compute ‚Ñì from next power of two
    if s.n == 0 { return Err(PiCcsError::InvalidInput("n=0 not allowed".into())); }
    let n_pad = s.n.next_power_of_two();
    let ell = n_pad.trailing_zeros() as usize;
    let d_sc = s.max_degree() as usize;

    let ext = params.extension_check(ell as u32, d_sc as u32)
        .map_err(|e| PiCcsError::ExtensionPolicyFailed(e.to_string()))?;
    tr.absorb_ccs_header(64, ext.s_supported, params.lambda, ell as u32, d_sc as u32, ext.slack_bits);

    // --- SECURITY: Absorb instance data BEFORE sampling challenges (match prover) ---
    tr.absorb_bytes(b"neo/ccs/instances");
    tr.absorb_u64(&[s.n as u64, s.m as u64, s.t() as u64]);
    // OPTIMIZATION: Absorb compact ZK-friendly digest instead of 500M+ field elements 
    // This reduces transcript absorption from ~51s to microseconds using Poseidon2
    let matrix_digest = digest_ccs_matrices(s);
    for &digest_elem in &matrix_digest {
        tr.absorb_f(&[F::from_u64(digest_elem.as_canonical_u64())]);
    }
    // CRITICAL: Absorb polynomial definition to prevent malicious polynomial substitution
    absorb_sparse_polynomial(tr, &s.f);
    
    // Absorb all instance data (commitment, public inputs, witness structure)
    for inst in mcs_list.iter() {
        // Absorb instance data that affects soundness
        tr.absorb_f(&inst.x);
        tr.absorb_u64(&[inst.m_in as u64]);
        // CRITICAL: Absorb commitment to prevent cross-instance attacks
        tr.absorb_f(&inst.c.data);
    }

    // Re-derive the SAME eq-binding vector and batching coefficients as the prover
    tr.absorb_bytes(b"neo/ccs/eq");
    let w_eq: Vec<K> = (0..ell).map(|_| tr.challenge_k()).collect();
    tr.absorb_bytes(b"neo/ccs/batch");
    let alphas: Vec<K> = (0..mcs_list.len()).map(|_| tr.challenge_k()).collect();
    
    let batch_coeffs = BatchingCoeffs { alphas };

    if proof.sumcheck_rounds.len() != ell { return Ok(false); }
    // Check sum-check rounds using shared helper (derives r and running_sum)
    // Enable R1CS eq-binding terminal check whenever the CCS is R1CS-shaped.
    let is_r1cs = is_r1cs_shape(s);
    let d_round = if is_r1cs { 3 } else { d_sc };
    let (r, running_sum, ok_rounds) =
        verify_sumcheck_rounds(tr, d_round, K::ZERO, &proof.sumcheck_rounds);
    if !ok_rounds { return Ok(false); }

    // === CRITICAL TRANSCRIPT BINDING SECURITY CHECK ===
    // Only apply transcript binding when we have sum-check rounds
    // For trivial cases (ell = 0, no rounds), skip binding checks
    if !proof.sumcheck_rounds.is_empty() {
        // Derive digest exactly where the prover did (after sum-check rounds)
        let digest = tr.state_digest();
        // Verify proof header matches transcript state
        if proof.header_digest != digest {
            eprintln!("‚ùå PI_CCS VERIFY: header digest mismatch (proof={:?}, verifier={:?})",
                      &proof.header_digest[..4], &digest[..4]);
            return Ok(false);
        }
        // Verify all output ME instances are bound to this transcript
        if !out_me.iter().all(|me| me.fold_digest == digest) {
            eprintln!("‚ùå PI_CCS VERIFY: out_me fold_digest mismatch");
            return Ok(false);
        }
    }

    // Light structural sanity: every output ME must carry the same r
    if !out_me.iter().all(|me| me.r == r) { return Ok(false); }

    // === CRITICAL BINDING: out_me[i] must match input instance ===
    // This prevents attacks where unrelated ME outputs pass RLC/DEC algebra
    if out_me.len() != mcs_list.len() { return Ok(false); }
    for (i, (out, inp)) in out_me.iter().zip(mcs_list.iter()).enumerate() {
        if out.c != inp.c { #[allow(unused_variables)] let _i = i; /* logs gated by workspace */ return Ok(false); }
        if out.m_in != inp.m_in { #[allow(unused_variables)] let _i = i; return Ok(false); }
        
        // Shape/consistency checks: catch subtle mismatches before terminal verification
        if out.X.rows() != neo_math::D { return Ok(false); }
        if out.X.cols() != inp.m_in { return Ok(false); }
        if out.y.len() != s.t() { return Ok(false); } // Number of CCS matrices
    }

    // === CRITICAL SUM-CHECK TERMINAL VERIFICATION ===
    // This is the missing piece that makes the proof sound!
    // We must verify that the final running_sum equals Q(r).
    // 
    // NOTE: Only CCS and range/decomp constraints in Q(r).
    // Tie constraints removed from sum-check as they break soundness.
    
    // Terminal check: generic CCS (t<3) uses Q(r) = Œ£ Œ±_i¬∑f(Y(r));
    // R1CS-style (t‚â•3) uses eq-binding: running_sum = eq(w,r)¬∑Œ£ Œ±_i¬∑(A(r)B(r)‚àíC(r)).
    if is_r1cs {
        // R1CS eq-bound terminal check
        if !out_me.iter().all(|me| me.y_scalars.len() == 3) { return Ok(false); }
        let mut wr = K::ONE;
        for (wi, ri) in w_eq.iter().zip(r.iter()) {
            wr *= (K::ONE - *wi) * (K::ONE - *ri) + *wi * *ri;
        }
        let mut expected = K::ZERO;
        for (inst_idx, me_inst) in out_me.iter().enumerate() {
            let a = me_inst.y_scalars[0];
            let b = me_inst.y_scalars[1];
            let c = me_inst.y_scalars[2];
            expected += batch_coeffs.alphas[inst_idx] * (a * b - c);
        }
        if running_sum != wr * expected { return Ok(false); }
    } else {
        // Generic CCS
        if !out_me.iter().all(|me| me.y_scalars.len() == s.t()) { return Ok(false); }
        let mut expected_q_r = K::ZERO;
        for (inst_idx, me_inst) in out_me.iter().enumerate() {
            let f_eval = s.f.eval_in_ext::<K>(&me_inst.y_scalars);
            expected_q_r += batch_coeffs.alphas[inst_idx] * f_eval;
        }
        if running_sum != expected_q_r { return Ok(false); }
    }

    // Optionally verify v_j = M_j^T œá_r if carried (disabled to keep verifier lightweight)

    Ok(true)
}
