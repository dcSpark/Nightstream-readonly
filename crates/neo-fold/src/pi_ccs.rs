//! Œ†_CCS: Sum-check reduction over extension field K
//!
//! This is the single sum-check used throughout Neo protocol.
//! Proves: Œ£_{u‚àà{0,1}^‚Ñì} (Œ£_i Œ±_i ¬∑ f_i(u)) ¬∑ œá_r(u) = 0

#![allow(non_snake_case)] // Allow mathematical notation like X, T, B

use crate::transcript::{FoldTranscript, Domain};
use crate::error::PiCcsError;
use neo_ccs::{CcsStructure, McsInstance, McsWitness, MeInstance, Mat, MatRef, SparsePoly};
use neo_ajtai::Commitment as Cmt;
use neo_math::{F, K};
use p3_field::{PrimeCharacteristicRing, Field, PrimeField64};
use rayon::prelude::*;
use p3_goldilocks::{Goldilocks, Poseidon2Goldilocks};
use p3_symmetric::Permutation;
use crate::sumcheck::{RoundOracle, run_sumcheck, run_sumcheck_skip_eval_at_one, verify_sumcheck_rounds, SumcheckOutput};

#[allow(dead_code)]
fn format_ext(x: K) -> String { format!("{:?}", x) }

/// Œ†_CCS proof containing the single sum-check over K
#[derive(Debug, Clone)]
pub struct PiCcsProof {
    /// Sum-check protocol rounds (univariate polynomials as coefficients)
    pub sumcheck_rounds: Vec<Vec<K>>,
    /// Extension policy binding digest  
    pub header_digest: [u8; 32],
    /// Precomputed v_j = M_j^T * œá_r vectors over K (one per matrix)
    pub vjs: Vec<Vec<K>>, 
}

// ===== CSR Sparse Matrix Operations =====

/// Minimal CSR (Compressed Sparse Row) format for sparse matrix operations
/// This enables O(nnz) operations instead of O(n*m) for our extremely sparse matrices
pub struct Csr<F> {
    pub rows: usize,
    pub cols: usize,
    pub indptr: Vec<usize>,  // len = rows + 1
    pub indices: Vec<usize>, // len = nnz  
    pub data: Vec<F>,        // len = nnz
}

/// Convert dense matrix to CSR format - O(nm) but done once
pub fn to_csr<F: Field + Copy>(m: &Mat<F>, rows: usize, cols: usize) -> Csr<F> {
    let mut indptr = Vec::with_capacity(rows + 1);
    let mut indices = Vec::new();
    let mut data = Vec::new();
    indptr.push(0);
    for r in 0..rows {
        let row = m.row(r);
        for (c, &v) in row.iter().enumerate() {
            if v != F::ZERO {
                indices.push(c);
                data.push(v);
            }
        }
        indptr.push(indices.len());
    }
    Csr { rows, cols, indptr, indices, data }
}

/// Sparse matrix-vector multiply: y = A * x  (CSR SpMV, O(nnz))
fn spmv_csr_ff<F: Field + Send + Sync + Copy>(a: &Csr<F>, x: &[F]) -> Vec<F> {
    let mut y = vec![F::ZERO; a.rows];
    y.par_iter_mut().enumerate().for_each(|(r, yr)| {
        let start = a.indptr[r];
        let end = a.indptr[r + 1];
        let mut acc = F::ZERO;
        for k in start..end {
            let c = a.indices[k];
            acc += a.data[k] * x[c];
        }
        *yr = acc;
    });
    y
}

// (dense œá_r variant removed; tests compute the dense check inline to avoid dead code)

// ===== Streaming/half-table equality-weighted CSR transpose multiply =====

/// Row weight provider: returns œá_r(row) or an equivalent row weight.
pub trait RowWeight: Sync {
    fn w(&self, row: usize) -> K;
}

/// Half-table implementation of œá_r row weights to avoid materializing the full tensor.
pub struct HalfTableEq {
    lo: Vec<K>,
    hi: Vec<K>,
    split: usize,
}

impl HalfTableEq {
    pub fn new(r: &[K]) -> Self {
        let ell = r.len();
        let split = ell / 2; // lower split bits in lo, higher in hi
        let lo_len = 1usize << split;
        let hi_len = 1usize << (ell - split);

        // Precompute factors (1-r_i, r_i)
        let mut one_minus = Vec::with_capacity(ell);
        for &ri in r { one_minus.push(K::ONE - ri); }

        // Build lo table
        let mut lo = vec![K::ONE; lo_len];
        for mask in 0..lo_len {
            let mut acc = K::ONE;
            let mut m = mask;
            for i in 0..split {
                let bit = m & 1;
                acc *= if bit == 0 { one_minus[i] } else { r[i] };
                m >>= 1;
            }
            lo[mask] = acc;
        }

        // Build hi table
        let mut hi = vec![K::ONE; hi_len];
        for mask in 0..hi_len {
            let mut acc = K::ONE;
            let mut m = mask;
            for j in 0..(ell - split) {
                let idx = split + j;
                let bit = m & 1;
                acc *= if bit == 0 { one_minus[idx] } else { r[idx] };
                m >>= 1;
            }
            hi[mask] = acc;
        }

        Self { lo, hi, split }
    }
}

impl RowWeight for HalfTableEq {
    #[inline]
    fn w(&self, row: usize) -> K {
        let lo_mask = (1usize << self.split) - 1;
        let lo_idx = row & lo_mask;
        let hi_idx = row >> self.split;
        self.lo[lo_idx] * self.hi[hi_idx]
    }
}

/// Weighted version of CSR transpose multiply: v = A^T * w, where w(row) is provided on-the-fly.
pub fn spmv_csr_t_weighted_fk<W: RowWeight + Sync>(a: &Csr<F>, w: &W) -> Vec<K> {
    let mut v = vec![K::ZERO; a.cols];
    let chunk = a.rows / rayon::current_num_threads().max(1) + 1;
    let partials: Vec<Vec<(usize, K)>> = (0..a.rows)
        .into_par_iter()
        .with_min_len(chunk)
        .map(|r| {
            let mut loc = Vec::with_capacity(a.indptr[r + 1] - a.indptr[r]);
            let wr = w.w(r);
            for k in a.indptr[r]..a.indptr[r + 1] {
                let c = a.indices[k];
                loc.push((c, K::from(a.data[k]) * wr));
            }
            loc
        })
        .collect();
    for loc in partials { for (c, val) in loc { v[c] += val; } }
    v
}


// ===== Matrix Digest Optimization =====

/// Compute canonical sparse digest of CCS matrices to avoid absorbing 500M+ field elements  
/// Uses ZK-friendly Poseidon2 hash with proper domain separation
fn digest_ccs_matrices<F: Field + PrimeField64>(s: &CcsStructure<F>) -> Vec<Goldilocks> {
    use rand_chacha::{ChaCha8Rng, rand_core::SeedableRng};
    
    // Use fixed seed for deterministic hashing (equivalent to derive_key concept)
    const CCS_DIGEST_SEED: u64 = 0x434353445F4D4154; // "CCSD_MAT" in hex
    let mut rng = ChaCha8Rng::seed_from_u64(CCS_DIGEST_SEED);
    let poseidon2 = Poseidon2Goldilocks::<16>::new_from_rng_128(&mut rng);
    
    // Sponge state for Poseidon2 (width=16)
    let mut state = [Goldilocks::ZERO; 16];
    let mut absorbed = 0;
    
    // PROPER DOMAIN SEPARATION: Absorb context string byte-by-byte (same as transcript pattern)
    const DOMAIN_STRING: &[u8] = b"neo/ccs/matrices/v1"; 
    for &byte in DOMAIN_STRING {
        if absorbed >= 15 { // Leave room for rate limiting
            poseidon2.permute_mut(&mut state);
            absorbed = 0;
        }
        state[absorbed] = Goldilocks::from_u32(byte as u32);
        absorbed += 1;
    }
    
    // Absorb matrix dimensions  
    if absorbed + 3 >= 16 { poseidon2.permute_mut(&mut state); absorbed = 0; }
    state[absorbed] = Goldilocks::from_u64(s.n as u64);
    state[absorbed + 1] = Goldilocks::from_u64(s.m as u64); 
    state[absorbed + 2] = Goldilocks::from_u64(s.t() as u64);
    // Note: absorbed will be reset to 0 in the loop below, so no need to track it here
    
    poseidon2.permute_mut(&mut state);
    
    // Absorb each matrix in sparse format
    for (j, matrix) in s.matrices.iter().enumerate() {
        // Reset and start with matrix index
        absorbed = 0;
        state[absorbed] = Goldilocks::from_u64(j as u64);
        absorbed += 1;
        
        let mat_ref = MatRef::from_mat(matrix);
        
        // Absorb sparse entries in canonical (row, col, val) order
        for row in 0..s.n {
            let row_slice = mat_ref.row(row);
            for (col, &val) in row_slice.iter().enumerate() {
                if val != F::ZERO {
                    if absorbed + 3 > 15 { // Leave room for rate limiting
                        poseidon2.permute_mut(&mut state);
                        absorbed = 0;
                    }
                    
                    // Absorb (row, col, val) as consecutive field elements
                    state[absorbed] = Goldilocks::from_u64(row as u64);
                    state[absorbed + 1] = Goldilocks::from_u64(col as u64);
                    state[absorbed + 2] = Goldilocks::from_u64(val.as_canonical_u64());
                    absorbed += 3;
                }
            }
        }
        
        // Permute after each matrix to ensure proper mixing
        poseidon2.permute_mut(&mut state);
    }
    
    // Return first 4 field elements as digest (128 bits of security)
    state[0..4].to_vec()
}

// ===== MLE Folding DP Helpers =====

/// Pad vector to power of 2 length with zeros
#[inline]
fn pad_to_pow2_k(mut v: Vec<K>, ell: usize) -> Result<Vec<K>, PiCcsError> {
    let want = 1usize << ell;
    if v.len() > want {
        return Err(PiCcsError::SumcheckError(format!(
            "Vector length {} exceeds 2^ell = {} - would silently truncate", 
            v.len(), want
        )));
    }
    v.resize(want, K::ZERO);
    Ok(v)
}

// ===== Blocked Parallel Matrix-Vector Multiplication =====

/// Absorb sparse polynomial definition into transcript for soundness binding.
/// This prevents a malicious prover from using different polynomials with the same matrix structure.
fn absorb_sparse_polynomial(tr: &mut FoldTranscript, f: &SparsePoly<F>) {
    // Absorb polynomial structure
    tr.absorb_bytes(b"neo/ccs/poly");
    tr.absorb_u64(&[f.arity() as u64]);
    tr.absorb_u64(&[f.terms().len() as u64]);
    
    // Absorb each term: coefficient + exponents (sorted for determinism)
    let mut terms: Vec<_> = f.terms().iter().collect();
    terms.sort_by_key(|term| &term.exps); // deterministic ordering
    
    for term in terms {
        tr.absorb_f(&[term.coeff]);
        tr.absorb_u64(&term.exps.iter().map(|&e| e as u64).collect::<Vec<_>>());
    }
}

// ===== Local MLE partial structures and Sum-check round oracles =====

// Generic CCS partials: one shrinking vector per matrix M_j z
struct MlePartials { s_per_j: Vec<Vec<K>> }
// R1CS residual partials: one shrinking vector over row-wise residuals
struct MleResiduals { s: Vec<K> }

struct GenericCcsOracle<'a> {
    s: &'a CcsStructure<F>,
    alphas: Vec<K>,
    partials_per_inst: Vec<MlePartials>,
    // Small-value fast path for round 0: access original F-vectors without lifting
    mz_f_per_inst: Vec<Vec<&'a [F]>>,
    ell: usize,
    d_sc: usize,
    first_round_done: bool,
}
impl<'a> RoundOracle for GenericCcsOracle<'a> {
    fn num_rounds(&self) -> usize { self.ell }
    fn degree_bound(&self) -> usize { self.d_sc }
    fn evals_at(&mut self, sample_xs: &[K]) -> Vec<K> {
        let t = self.s.t();
        let mut sample_ys = vec![K::ZERO; sample_xs.len()];
        if !self.first_round_done {
            // Round-0 small-value path: accumulate in F, then lift once per sample X
            for (inst_idx, mz_f) in self.mz_f_per_inst.iter().enumerate() {
                let mut a_f = vec![F::ZERO; t];
                let mut d_f = vec![F::ZERO; t];
                let n_pad = 1usize << self.ell;
                let half = n_pad >> 1;
                for j in 0..t {
                    let v_f = mz_f[j];
                    let mut aj = F::ZERO;
                    let mut dj = F::ZERO;
                    for k in 0..half {
                        let idx_e = 2*k;
                        let idx_o = idx_e + 1;
                        let e = if idx_e < v_f.len() { v_f[idx_e] } else { F::ZERO };
                        let o = if idx_o < v_f.len() { v_f[idx_o] } else { F::ZERO };
                        aj += e;
                        dj += o - e;
                    }
                    a_f[j] = aj; d_f[j] = dj;
                }
                let alpha = self.alphas[inst_idx];
                let mut y_buf = vec![K::ZERO; t];
                for (sx, &X) in sample_xs.iter().enumerate() {
                    for j in 0..t { y_buf[j] = K::from(a_f[j]) + K::from(d_f[j]) * X; }
                    let f_eval = self.s.f.eval_in_ext::<K>(&y_buf);
                    sample_ys[sx] += alpha * f_eval;
                }
            }
            self.first_round_done = true;
        } else {
            for (inst_idx, partials) in self.partials_per_inst.iter().enumerate() {
                let mut a = vec![K::ZERO; t];
                let mut delta = vec![K::ZERO; t];
                for j in 0..t {
                    let v = &partials.s_per_j[j];
                    debug_assert!(v.len().is_power_of_two() && v.len() >= 2);
                    let half = v.len() >> 1;
                    let (mut aj, mut dj) = (K::ZERO, K::ZERO);
                    for k in 0..half {
                        let e = v[2*k];
                        let o = v[2*k + 1];
                        aj += e;
                        dj += o - e;
                    }
                    a[j] = aj; delta[j] = dj;
                }
                let alpha = self.alphas[inst_idx];
                let mut y_buf = vec![K::ZERO; t];
                for (sx, &X) in sample_xs.iter().enumerate() {
                    for j in 0..t { y_buf[j] = a[j] + delta[j] * X; }
                    let f_eval = self.s.f.eval_in_ext::<K>(&y_buf);
                    sample_ys[sx] += alpha * f_eval;
                }
            }
        }
        sample_ys
    }
    fn fold(&mut self, r_i: K) {
        for partials in self.partials_per_inst.iter_mut() {
            for v in &mut partials.s_per_j {
                let n2 = v.len() >> 1;
                for k in 0..n2 {
                    let a0 = v[2*k]; let b0 = v[2*k + 1];
                    v[k] = (K::ONE - r_i) * a0 + r_i * b0;
                }
                v.truncate(n2);
            }
        }
    }
}

struct R1csResidualOracle {
    alphas: Vec<K>,
    residuals_per_inst: Vec<MleResiduals>,
    ell: usize,
    d_sc: usize,
}
impl RoundOracle for R1csResidualOracle {
    fn num_rounds(&self) -> usize { self.ell }
    fn degree_bound(&self) -> usize { self.d_sc }
    fn evals_at(&mut self, sample_xs: &[K]) -> Vec<K> {
        let mut sample_ys = vec![K::ZERO; sample_xs.len()];
        for (inst_idx, partials) in self.residuals_per_inst.iter().enumerate() {
            let v = &partials.s;
            debug_assert!(v.len().is_power_of_two() && v.len() >= 2);
            let half = v.len() >> 1;
            let (mut a_res, mut d_res) = (K::ZERO, K::ZERO);
            for k in 0..half {
                let e = v[2*k]; let o = v[2*k + 1];
                a_res += e; d_res += o - e;
            }
            let alpha = self.alphas[inst_idx];
            for (sx, &X) in sample_xs.iter().enumerate() {
                sample_ys[sx] += alpha * (a_res + d_res * X);
            }
        }
        sample_ys
    }
    fn fold(&mut self, r_i: K) {
        for partials in self.residuals_per_inst.iter_mut() {
            let v = &mut partials.s;
            let n2 = v.len() >> 1;
            for k in 0..n2 {
                let a0 = v[2*k]; let b0 = v[2*k + 1];
                v[k] = (K::ONE - r_i) * a0 + r_i * b0;
            }
            v.truncate(n2);
        }
    }
}

/// Batching coefficients for the composed polynomial Q
/// 
/// In v1: Only CCS constraints are included in Q(u).
/// Range and evaluation tie constraints are handled outside the sum-check.
#[derive(Debug, Clone)]
struct BatchingCoeffs {
    /// Œ± coefficients for CCS constraints f(Mz)  
    alphas: Vec<K>,
}

/// Evaluate range/decomposition constraint polynomials NC_i(z,Z) at point u.
/// These assert: Z = Decomp_b(z) and ||Z||_‚àû < b
/// 
/// NOTE: For honest instances where Z == Decomp_b(z) and ||Z||_‚àû < b, 
///       this MUST return zero to make the composed polynomial Q sum to zero.
pub fn eval_range_decomp_constraints(
    z: &[F],
    Z: &neo_ccs::Mat<F>,
    _u: &[K],                  // not used: constraints are independent of u
    params: &neo_params::NeoParams,
) -> K {
    // REAL CONSTRAINT EVALUATION (degree 0 in u)
    // Enforces two facts:
    // 1. Decomposition correctness: z[c] = Œ£_{i=0}^{d-1} b^i * Z[i,c] 
    // 2. Digit range (balanced): R_b(x) = x * ‚àè_{t=1}^{b-1} (x-t)(x+t) = 0
    
    let d = Z.rows();
    let m = Z.cols();

    // Sanity: shapes
    if z.len() != m {
        // Treat shape mismatch as a hard violation: contribute a non-zero sentinel.
        return K::from(F::ONE);
    }

    // Precompute base powers in F for recomposition
    let b_f = F::from_u64(params.b as u64);
    let mut pow_b = vec![F::ONE; d];
    for i in 1..d { 
        pow_b[i] = pow_b[i - 1] * b_f; 
    }

    // === (A) Decomposition correctness residual: sum of squares in K ===
    let mut decomp_residual = K::ZERO;
    for c in 0..m {
        // z_rec = Œ£_{i=0..d-1} (b^i * Z[i,c])
        let mut z_rec_f = F::ZERO;
        for i in 0..d {
            z_rec_f += Z[(i, c)] * pow_b[i];
        }
        // Residual (in K): (z_rec - z[c])^2
        let diff_k = K::from(z_rec_f) - K::from(z[c]);
        decomp_residual += diff_k * diff_k;
    }

    // === (B) Range residual: R_b(x) = x * ‚àè_{t=1}^{b-1} (x - t)(x + t) for every digit ===
    // Works for the "Balanced" digit set in your code path; if you ever switch styles,
    // generalize this polynomial accordingly.
    let mut range_residual = K::ZERO;

    // Precompute constants in F for 1..(b-1)
    let mut t_vals = Vec::with_capacity((params.b - 1) as usize);
    for t in 1..params.b {
        t_vals.push(F::from_u64(t as u64));
    }

    for c in 0..m {
        for i in 0..d {
            let x = Z[(i, c)];            // digit in F
            // Build R_b(x) in K
            let mut rbx = K::from(x);     // starts with the leading x factor
            // Multiply (x - t)(x + t) over t=1..b-1
            for &t in &t_vals {
                rbx *= K::from(x) - K::from(t);   // (x - t)
                rbx *= K::from(x) + K::from(t);   // (x + t)
            }
            range_residual += rbx;
        }
    }

    decomp_residual + range_residual
}

/// Evaluate tie constraint polynomials ‚ü®M_j^T œá_u, Z‚ü© - y_j at point u.
/// These assert that the y_j values are consistent with Z and the random point.
/// 
/// CRITICAL: This must be implemented correctly for soundness!
/// The sum-check terminal verification depends on this being accurate.
pub fn eval_tie_constraints(
    s: &CcsStructure<F>,
    Z: &neo_ccs::Mat<F>,
    claimed_y: &[Vec<K>], // y_j entries as produced in ME (length t, each length d)
    u: &[K],
) -> K {
    // REAL MULTILINEAR EXTENSION EVALUATION
    // Implements: Œ£_j Œ£_œÅ (‚ü®Z_œÅ,*, M_j^T œá_u‚ü© - y_{j,œÅ})
    
    // œá_u ‚àà K^n
    let chi_u = neo_ccs::utils::tensor_point::<K>(u);

    let d = Z.rows();       // Ajtai dimension
    let m = Z.cols();       // number of columns in Z (== s.m)

    debug_assert_eq!(m, s.m, "Z.cols() must equal s.m");
    
    // If claimed_y is missing or has wrong shape, we conservatively treat it as zero.
    // (This will force the prover's Q(u) to carry the full ‚ü®Z, M_j^T œá_u‚ü© mass, which
    // then must cancel at r when the real y_j are used.)
    let safe_y = |j: usize, rho: usize| -> K {
        if j < claimed_y.len() && rho < claimed_y[j].len() {
            claimed_y[j][rho]
        } else {
            K::ZERO
        }
    };

    let mut total = K::ZERO;

    // For each matrix M_j, build v_j(u) = M_j^T œá_u ‚àà K^m, then compute Z * v_j(u) ‚àà K^d
    for (j, mj) in s.matrices.iter().enumerate() {
        // v_j[c] = Œ£_{row=0..n-1} M_j[row,c] * œá_u[row]
        let mut vj = vec![K::ZERO; s.m];
        for row in 0..s.n {
            let coeff = chi_u[row];
            let mj_row = mj.row(row);
            for c in 0..s.m {
                vj[c] += K::from(mj_row[c]) * coeff;
            }
        }
        // lhs = Z * v_j(u) as a length-d K-vector
        let z_ref = neo_ccs::MatRef::from_mat(Z);
        let lhs = neo_ccs::utils::mat_vec_mul_fk::<F, K>(z_ref.data, z_ref.rows, z_ref.cols, &vj);

        // Accumulate the residual (lhs - y_j)
        for rho in 0..d {
            total += lhs[rho] - safe_y(j, rho);
        }
    }

    total
}

/// Prove Œ†_CCS: CCS instances satisfy constraints via sum-check
/// 
/// Input: k+1 CCS instances, outputs k ME instances + proof
pub fn pi_ccs_prove<L: neo_ccs::traits::SModuleHomomorphism<F, Cmt>>(
    tr: &mut FoldTranscript,
    params: &neo_params::NeoParams,
    s: &CcsStructure<F>,
    mcs_list: &[McsInstance<Cmt, F>],
    witnesses: &[McsWitness<F>],
    l: &L, // we need L to check c = L(Z) and to compute X = L_x(Z)
) -> Result<(Vec<MeInstance<Cmt, F, K>>, PiCcsProof), PiCcsError> {
    tr.domain(Domain::CCS);

    // --- Input & policy checks ---
    if mcs_list.is_empty() || mcs_list.len() != witnesses.len() {
        return Err(PiCcsError::InvalidInput("empty or mismatched inputs".into()));
    }
    // >>> CHANGE #1: allow arbitrary n; compute ‚Ñì from next power of two
    if s.n == 0 {
        return Err(PiCcsError::InvalidInput("n=0 not allowed".into()));
    }
    let n_pad = s.n.next_power_of_two();
    let ell = n_pad.trailing_zeros() as usize;
    let d_sc = s.max_degree() as usize;

    let ext = params.extension_check(ell as u32, d_sc as u32)
        .map_err(|e| PiCcsError::ExtensionPolicyFailed(format!("Extension policy validation failed: {}", e)))?;
    // Enforce strict security policy: slack_bits must be non-negative
    // This ensures we meet or exceed the target lambda-bit security level
    if ext.slack_bits < 0 {
        return Err(PiCcsError::ExtensionPolicyFailed(format!(
            "Insufficient security slack: {} bits (need ‚â• 0 for target {}-bit security)", 
            ext.slack_bits, params.lambda
        )));
    }
    tr.absorb_ccs_header(64, ext.s_supported, params.lambda, ell as u32, d_sc as u32, ext.slack_bits);

    // MAJOR OPTIMIZATION: Convert to CSR sparse format once for all operations
    // This enables O(nnz) operations instead of O(n*m) for our extremely sparse matrices  
    let csr_start = std::time::Instant::now();
    let mats_csr: Vec<Csr<F>> = s.matrices.iter().map(|m| to_csr::<F>(m, s.n, s.m)).collect();
    let total_nnz: usize = mats_csr.iter().map(|c| c.data.len()).sum();
    println!("üî• [NUCLEAR] CSR conversion completed: {:.2}ms ({} matrices, {} nnz total, {:.4}% density)",
             csr_start.elapsed().as_secs_f64() * 1000.0, 
             mats_csr.len(), total_nnz, 
             (total_nnz as f64) / (s.n * s.m * s.matrices.len()) as f64 * 100.0);

    // --- Prepare per-instance data and check c=L(Z) ---
    // Also build z = x||w and cache M_j z over F for each instance.
    struct Inst<'a> {
        Z: &'a Mat<F>, 
        m_in: usize, 
        mz: Vec<Vec<F>>,
        c: Cmt,
    }
    let mut insts: Vec<Inst> = Vec::with_capacity(mcs_list.len());
    let instance_prep_start = std::time::Instant::now();
    for (inst_idx, (inst, wit)) in mcs_list.iter().zip(witnesses.iter()).enumerate() {
        let z_check_start = std::time::Instant::now();
        let z = neo_ccs::relations::check_mcs_opening(l, inst, wit)
            .map_err(|e| PiCcsError::InvalidInput(format!("MCS opening failed: {e}")))?;
        println!("üîß [INSTANCE {}] MCS opening check: {:.2}ms", inst_idx, 
                 z_check_start.elapsed().as_secs_f64() * 1000.0);
        // SECURITY: Ensure z matches CCS width to prevent OOB during M*z
        if z.len() != s.m {
            return Err(PiCcsError::InvalidInput(format!(
                "SECURITY: z length {} does not match CCS column count {} (malformed instance)",
                z.len(), s.m
            )));
        }
        
        // === CRITICAL SECURITY CHECK: Z == Decomp_b(z) ===
        // This prevents prover from using satisfying z for CCS but different Z for commitment
        let decomp_start = std::time::Instant::now();
        let Z_expected_col_major = neo_ajtai::decomp_b(&z, params.b, neo_math::D, neo_ajtai::DecompStyle::Balanced);
        println!("üîß [INSTANCE {}] Decomp_b: {:.2}ms", inst_idx, 
                 decomp_start.elapsed().as_secs_f64() * 1000.0);
        
        let range_check_start = std::time::Instant::now();
        neo_ajtai::assert_range_b(&Z_expected_col_major, params.b)
            .map_err(|e| PiCcsError::InvalidInput(format!("Range check failed on expected Z: {e}")))?;
        println!("üîß [INSTANCE {}] Range check: {:.2}ms", inst_idx, 
                 range_check_start.elapsed().as_secs_f64() * 1000.0);
        
        // Convert Z_expected from column-major to row-major format to match wit.Z
        let format_conv_start = std::time::Instant::now();
        let d = neo_math::D;
        let m = Z_expected_col_major.len() / d;
        let mut Z_expected_row_major = vec![neo_math::F::ZERO; d * m];
        for col in 0..m {
            for row in 0..d {
                let col_major_idx = col * d + row;
                let row_major_idx = row * m + col;
                Z_expected_row_major[row_major_idx] = Z_expected_col_major[col_major_idx];
            }
        }
        println!("üîß [INSTANCE {}] Format conversion: {:.2}ms", inst_idx, 
                 format_conv_start.elapsed().as_secs_f64() * 1000.0);
        
        // Compare Z with expected decomposition (both in row-major format)
        let z_compare_start = std::time::Instant::now();
        if wit.Z.as_slice() != Z_expected_row_major.as_slice() {
            return Err(PiCcsError::InvalidInput("SECURITY: Z != Decomp_b(z) - prover using inconsistent z and Z".into()));
        }
        println!("üîß [INSTANCE {}] Z comparison: {:.2}ms", inst_idx, 
                 z_compare_start.elapsed().as_secs_f64() * 1000.0);
        // MAJOR OPTIMIZATION: Use CSR sparse matrix-vector multiply - O(nnz) instead of O(n*m)!
        let mz_start = std::time::Instant::now();
        let mz: Vec<Vec<F>> = mats_csr.par_iter().map(|csr| 
            spmv_csr_ff::<F>(csr, &z)
        ).collect();
        println!("üí• [TIMING] CSR M_j z computation: {:.2}ms (nnz={}, vs {}M dense elements - {}x reduction)", 
                 mz_start.elapsed().as_secs_f64() * 1000.0, total_nnz, 
                 (s.n * s.m * s.matrices.len()) / 1_000_000,
                 (s.n * s.m * s.matrices.len()) / total_nnz.max(1));
        insts.push(Inst{ Z: &wit.Z, m_in: inst.m_in, mz, c: inst.c.clone() });
    }
    println!("üîß [TIMING] Instance preparation total: {:.2}ms ({} instances)", 
             instance_prep_start.elapsed().as_secs_f64() * 1000.0, insts.len());

    // --- SECURITY: Absorb instance data BEFORE sampling challenges to prevent malleability ---
    let transcript_start = std::time::Instant::now();
    tr.absorb_bytes(b"neo/ccs/instances");
    tr.absorb_u64(&[s.n as u64, s.m as u64, s.t() as u64]);
    // OPTIMIZATION: Absorb compact ZK-friendly digest instead of 500M+ field elements 
    // This reduces transcript absorption from ~51s to microseconds using Poseidon2
    let matrix_digest = digest_ccs_matrices(s);
    for &digest_elem in &matrix_digest {
        tr.absorb_f(&[F::from_u64(digest_elem.as_canonical_u64())]);
    }
    // CRITICAL: Absorb polynomial definition to prevent malicious polynomial substitution
    absorb_sparse_polynomial(tr, &s.f);
    
    // Absorb all instance data (commitment, public inputs, witness structure)
    for inst in mcs_list.iter() {
        // Absorb instance data that affects soundness
        tr.absorb_f(&inst.x);
        tr.absorb_u64(&[inst.m_in as u64]);
        // CRITICAL: Absorb commitment to prevent cross-instance attacks
        tr.absorb_f(&inst.c.data);
    }
    println!("üîß [TIMING] Transcript absorption: {:.2}ms", 
             transcript_start.elapsed().as_secs_f64() * 1000.0);

    // --- Generate batching coefficients for composed polynomial Q ---
    let batching_start = std::time::Instant::now();
    tr.absorb_bytes(b"neo/ccs/batch");
    
    // Œ± coefficients for CCS constraints (one per instance)
    let alphas: Vec<K> = (0..insts.len()).map(|_| tr.challenge_k()).collect();
    
    let batch_coeffs = BatchingCoeffs { alphas };
    println!("üîß [TIMING] Batching coefficients: {:.2}ms", 
             batching_start.elapsed().as_secs_f64() * 1000.0);

    // --- Run sum-check rounds over composed polynomial Q (degree ‚â§ d_sc) ---
    println!("üîç Sum-check starting: {} instances, {} rounds", insts.len(), ell);
    let sample_xs: Vec<K> = (0..=d_sc as u64).map(|u| K::from(F::from_u64(u))).collect();
    
    // TODO: Implement product-binding check for R1CS
    // SOUNDNESS: until we add a product-binding check for R1CS,
    // force the generic CCS oracle for all t.
    let use_generic_ccs = true;

    // Build partial states (invariant across the engine)
    let mle_start = std::time::Instant::now();
    let partials_per_inst_opt: Option<Vec<MlePartials>>;
    let residuals_per_inst_opt: Option<Vec<MleResiduals>>;
    if use_generic_ccs {
        let partials: Result<Vec<MlePartials>, PiCcsError> = insts.par_iter().map(|inst| {
            let mut s_per_j = Vec::with_capacity(s.t());
            for j in 0..s.t() {
                let w_k: Vec<K> = inst.mz[j].iter().map(|&x| K::from(x)).collect();
                let w_k = pad_to_pow2_k(w_k, ell)?;
                s_per_j.push(w_k);
            }
            Ok(MlePartials { s_per_j })
        }).collect();
        partials_per_inst_opt = Some(partials?);
        residuals_per_inst_opt = None;
    } else {
        let residuals: Result<Vec<MleResiduals>, PiCcsError> = insts.par_iter().map(|inst| {
            let az = &inst.mz[0]; let bz = &inst.mz[1]; let cz = &inst.mz[2];
            let mut row_residuals: Vec<K> = Vec::with_capacity(s.n);
            for i in 0..s.n {
                let a = K::from(az[i]); let b = K::from(bz[i]); let c = K::from(cz[i]);
                row_residuals.push(a * b - c);
            }
            let s_vec = pad_to_pow2_k(row_residuals, ell)?;
            Ok(MleResiduals { s: s_vec })
        }).collect();
        residuals_per_inst_opt = Some(residuals?);
        partials_per_inst_opt = None;
    }
    println!("üîß [TIMING] MLE partials setup: {:.2}ms",
             mle_start.elapsed().as_secs_f64() * 1000.0);

    // Drive rounds with the generic engine
    let initial_sum = K::ZERO;
    let SumcheckOutput { rounds, challenges: r, final_sum: _running_sum } = if use_generic_ccs {
        let mut oracle = GenericCcsOracle {
            s, alphas: batch_coeffs.alphas.clone(),
            partials_per_inst: partials_per_inst_opt.unwrap(),
            mz_f_per_inst: insts.iter().map(|inst| inst.mz.iter().map(|v| v.as_slice()).collect()).collect(),
            ell, d_sc,
            first_round_done: false,
        };
        if d_sc >= 1 { run_sumcheck_skip_eval_at_one(tr, &mut oracle, initial_sum, &sample_xs)? }
        else { run_sumcheck(tr, &mut oracle, initial_sum, &sample_xs)? }
    } else {
        let mut oracle = R1csResidualOracle {
            alphas: batch_coeffs.alphas.clone(),
            residuals_per_inst: residuals_per_inst_opt.unwrap(),
            ell, d_sc,
        };
        if d_sc >= 1 { run_sumcheck_skip_eval_at_one(tr, &mut oracle, initial_sum, &sample_xs)? }
        else { run_sumcheck(tr, &mut oracle, initial_sum, &sample_xs)? }
    };

    println!("üîß [TIMING] Sum-check rounds complete ({} rounds)", ell);

    // Compute M_j^T * œá_r using streaming/half-table weights (no full œá_r materialization)
    println!("üöÄ [OPTIMIZATION] Computing M_j^T * œá_r with half-table weights...");
    let transpose_once_start = std::time::Instant::now();
    let w = HalfTableEq::new(&r);
    let vjs: Vec<Vec<K>> = mats_csr.par_iter()
        .map(|csr| spmv_csr_t_weighted_fk::<_>(csr, &w))
        .collect();
    println!("üí• [OPTIMIZATION] Weighted CSR M_j^T * œá_r computed: {:.2}ms (nnz={})",
             transpose_once_start.elapsed().as_secs_f64() * 1000.0, total_nnz);

    // --- Build ME instances (one per input) ---
    let me_start = std::time::Instant::now();
    
    // CRITICAL SECURITY FIX: Generate fold_digest from final transcript state
    // This binds the ME instances to the exact folding proof and prevents re-binding attacks
    let fold_digest = tr.state_digest();
    
    let mut out_me = Vec::with_capacity(insts.len());
    for (inst_idx, inst) in insts.iter().enumerate() {
        // X = L_x(Z)
        let X = l.project_x(inst.Z, inst.m_in);
        
        // OPTIMIZATION: Use precomputed v_j vectors and MLE fold results  
        let mut y = Vec::with_capacity(s.t());
        // Use final MLE fold results for Y_j(r) (already computed above) ‚Äî we do not maintain per-j partials
        // here, so reconstruct Y_j(r) via sparse transpose multiply below.
        // We still populate y_scalars with the CORRECT scalars: ‚ü®(M_j z), œá_r‚ü© derived below.
        
        // Use precomputed v_j = M_j^T * œá_r vectors (no more expensive recomputation per instance!)
        let z_operations_start = std::time::Instant::now();
        for (_j, vj) in vjs.iter().enumerate() {
            let z_ref = neo_ccs::MatRef::from_mat(inst.Z);
            let yj = neo_ccs::utils::mat_vec_mul_fk::<F,K>(z_ref.data, z_ref.rows, z_ref.cols, vj);
            y.push(yj);
        }
        println!("üöÄ [OPTIMIZATION] Used precomputed v_j vectors - only Z * v_j needed: {:.2}ms", 
                 z_operations_start.elapsed().as_secs_f64() * 1000.0);
        
        // Compute the CORRECT Y_j(r) scalars: ‚ü®(M_j z), œá_r‚ü© using streaming weights
        let y_scalars: Vec<K> = (0..s.t()).map(|j| {
            let mut acc = K::ZERO;
            for i in 0..s.n {
                acc += K::from(insts[inst_idx].mz[j][i]) * w.w(i);
            }
            acc
        }).collect();

        out_me.push(MeInstance{ 
            c_step_coords: vec![], // Pattern B: Populated by IVC layer, not folding
            u_offset: 0,  // Pattern B: Unused (computed deterministically from witness structure)
            u_len: 0,     // Pattern B: Unused (computed deterministically from witness structure)
            c: inst.c.clone(), 
            X, 
            r: r.clone(), 
            y, 
            y_scalars, // SECURITY: Correct scalars for terminal check
            m_in: inst.m_in,
            fold_digest, // Bind to transcript
        });
    }

    println!("üîß [TIMING] ME instance building: {:.2}ms", 
             me_start.elapsed().as_secs_f64() * 1000.0);

    let proof = PiCcsProof { sumcheck_rounds: rounds, header_digest: fold_digest, vjs };
    Ok((out_me, proof))
}

/// Verify Œ†_CCS: Check sum-check rounds AND the critical final claim Q(r) = 0
pub fn pi_ccs_verify(
    tr: &mut FoldTranscript,
    params: &neo_params::NeoParams,
    s: &CcsStructure<F>,
    mcs_list: &[McsInstance<Cmt, F>], // Now used for final Q(r) check
    out_me: &[MeInstance<Cmt, F, K>],
    proof: &PiCcsProof,
) -> Result<bool, PiCcsError> {
    tr.domain(Domain::CCS);
    // >>> CHANGE #2: allow arbitrary n; compute ‚Ñì from next power of two
    if s.n == 0 { return Err(PiCcsError::InvalidInput("n=0 not allowed".into())); }
    let n_pad = s.n.next_power_of_two();
    let ell = n_pad.trailing_zeros() as usize;
    let d_sc = s.max_degree() as usize;

    let ext = params.extension_check(ell as u32, d_sc as u32)
        .map_err(|e| PiCcsError::ExtensionPolicyFailed(e.to_string()))?;
    tr.absorb_ccs_header(64, ext.s_supported, params.lambda, ell as u32, d_sc as u32, ext.slack_bits);

    // --- SECURITY: Absorb instance data BEFORE sampling challenges (match prover) ---
    tr.absorb_bytes(b"neo/ccs/instances");
    tr.absorb_u64(&[s.n as u64, s.m as u64, s.t() as u64]);
    // OPTIMIZATION: Absorb compact ZK-friendly digest instead of 500M+ field elements 
    // This reduces transcript absorption from ~51s to microseconds using Poseidon2
    let matrix_digest = digest_ccs_matrices(s);
    for &digest_elem in &matrix_digest {
        tr.absorb_f(&[F::from_u64(digest_elem.as_canonical_u64())]);
    }
    // CRITICAL: Absorb polynomial definition to prevent malicious polynomial substitution
    absorb_sparse_polynomial(tr, &s.f);
    
    // Absorb all instance data (commitment, public inputs, witness structure)
    for inst in mcs_list.iter() {
        // Absorb instance data that affects soundness
        tr.absorb_f(&inst.x);
        tr.absorb_u64(&[inst.m_in as u64]);
        // CRITICAL: Absorb commitment to prevent cross-instance attacks
        tr.absorb_f(&inst.c.data);
    }

    // Re-derive the SAME batching coefficients as the prover
    tr.absorb_bytes(b"neo/ccs/batch");
    let alphas: Vec<K> = (0..mcs_list.len()).map(|_| tr.challenge_k()).collect();
    
    let batch_coeffs = BatchingCoeffs { alphas };

    if proof.sumcheck_rounds.len() != ell { return Ok(false); }
    // Check sum-check rounds using shared helper (derives r and running_sum)
    let (r, running_sum, ok_rounds) =
        verify_sumcheck_rounds(tr, d_sc, K::ZERO, &proof.sumcheck_rounds);
    if !ok_rounds { return Ok(false); }

    // === CRITICAL TRANSCRIPT BINDING SECURITY CHECK ===
    // Only apply transcript binding when we have sum-check rounds
    // For trivial cases (ell = 0, no rounds), skip binding checks
    if !proof.sumcheck_rounds.is_empty() {
        // Derive digest exactly where the prover did (after sum-check rounds)
        let digest = tr.state_digest();
        // Verify proof header matches transcript state
        if proof.header_digest != digest {
            eprintln!("‚ùå PI_CCS VERIFY: header digest mismatch (proof={:?}, verifier={:?})",
                      &proof.header_digest[..4], &digest[..4]);
            return Ok(false);
        }
        // Verify all output ME instances are bound to this transcript
        if !out_me.iter().all(|me| me.fold_digest == digest) {
            eprintln!("‚ùå PI_CCS VERIFY: out_me fold_digest mismatch");
            return Ok(false);
        }
    }

    // Light structural sanity: every output ME must carry the same r
    if !out_me.iter().all(|me| me.r == r) { return Ok(false); }

    // === CRITICAL BINDING: out_me[i] must match input instance ===
    // This prevents attacks where unrelated ME outputs pass RLC/DEC algebra
    if out_me.len() != mcs_list.len() { return Ok(false); }
    for (out, inp) in out_me.iter().zip(mcs_list.iter()) {
        if out.c != inp.c { return Ok(false); }
        if out.m_in != inp.m_in { return Ok(false); }
        
        // Shape/consistency checks: catch subtle mismatches before terminal verification
        if out.X.rows() != neo_math::D { return Ok(false); }
        if out.X.cols() != inp.m_in { return Ok(false); }
        if out.y.len() != s.t() { return Ok(false); } // Number of CCS matrices
    }

    // === CRITICAL SUM-CHECK TERMINAL VERIFICATION ===
    // This is the missing piece that makes the proof sound!
    // We must verify that the final running_sum equals Q(r).
    // 
    // NOTE: Only CCS and range/decomp constraints in Q(r).
    // Tie constraints removed from sum-check as they break soundness.
    
    // Terminal check: Only safe when Q(r) can be computed from y_scalars alone.
    // For generic CCS with t < 3 (no element-wise product), Q(r) = Œ£ Œ±_i ¬∑ f(Y_i(r)).
    // For R1CS-style (t ‚â• 3), ‚ü®Az‚àòBz, œá_r‚ü© cannot be derived from y_scalars; skip here.
    if s.t() < 3 {
        if !out_me.iter().all(|me| me.y_scalars.len() == s.t()) { return Ok(false); }
        let mut expected_q_r = K::ZERO;
        for (inst_idx, me_inst) in out_me.iter().enumerate() {
            let f_eval = s.f.eval_in_ext::<K>(&me_inst.y_scalars);
            expected_q_r += batch_coeffs.alphas[inst_idx] * f_eval;
        }
        if running_sum != expected_q_r { return Ok(false); }
    }

    // Verify v_j = M_j^T œá_r if provided
    if !proof.vjs.is_empty() {
        let mats_csr: Vec<Csr<F>> = s.matrices.iter().map(|m| to_csr::<F>(m, s.n, s.m)).collect();
        let w = HalfTableEq::new(&r);
        let vjs_hat: Vec<Vec<K>> = mats_csr.par_iter().map(|csr| spmv_csr_t_weighted_fk::<_>(csr, &w)).collect();
        if proof.vjs.len() != vjs_hat.len() { return Ok(false); }
        for (lhs, rhs) in proof.vjs.iter().zip(vjs_hat.iter()) {
            if lhs.len() != rhs.len() || !lhs.iter().zip(rhs).all(|(a,b)| *a == *b) { return Ok(false); }
        }
    }

    Ok(true)
}
